__NUXT_JSONP__("/articles/bloveless/589879", (function(a,b,c,d,e,f){c.type_of="article";c.id=589879;c.title="Logging V Monitoring: Part 1";c.description="Introduction   What do you do when your application is down? Better yet: How can you predict...";c.readable_publish_date="Feb 3";c.slug="logging-v-monitoring-part-1-47lk";c.path="\u002Fbloveless\u002Flogging-v-monitoring-part-1-47lk";c.url=d;c.comments_count=0;c.public_reactions_count=e;c.collection_id=a;c.published_timestamp=b;c.positive_reactions_count=e;c.cover_image="https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--InCBrKe8--\u002Fc_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fgsmymnhokhosjlnieb18.jpg";c.social_image="https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--x6iBlgtt--\u002Fc_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fgsmymnhokhosjlnieb18.jpg";c.canonical_url=d;c.created_at="2021-02-02T13:40:22Z";c.edited_at="2021-02-03T13:01:09Z";c.crossposted_at=a;c.published_at=b;c.last_comment_at=b;c.reading_time_minutes=7;c.tag_list="logging, monitoring, grafana, alerts";c.tags=["logging","monitoring","grafana","alerts"];c.body_html="\u003Ch2\u003E\n  \u003Ca name=\"introduction\" href=\"#introduction\" class=\"anchor\"\u003E\n  \u003C\u002Fa\u003E\n  Introduction\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EWhat do you do when your application is down? Better yet: How can you \u003Cem\u003Epredict\u003C\u002Fem\u003E when your application may go down? How do you begin an investigation in the most efficient way possible and resolve issues quickly?\u003C\u002Fp\u003E\n\n\u003Cp\u003EUnderstanding the difference between logging and monitoring is critical, and can make all the difference in your ability to trace issues back to their root cause. If you confuse the two or use one without the other, you’re setting yourself up for long nights and weekends debugging your app.\u003C\u002Fp\u003E\n\n\u003Cp\u003EIn this article, we’ll look at how to effectively log and monitor your systems. I’ll tell you about a few good practices that I’ve learned over the years and some interesting metrics that you may want to monitor in your systems. Finally, I’ll show you a small web application that had no monitoring, alerting, or logging. I’ll demonstrate how I fixed the logging and how I’ve implemented monitoring and alerting around those logs.\u003C\u002Fp\u003E\n\n\u003Cp\u003EEveryone has some sort of logging in their applications, even if it’s just writing to a file to review later. By the end of this article, I hope to convince you that logging without monitoring is about as good as no logging at all. Along the way, we can review some best practices for becoming a better logger.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"logging-vs-monitoring\" href=\"#logging-vs-monitoring\" class=\"anchor\"\u003E\n  \u003C\u002Fa\u003E\n  Logging vs Monitoring\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EFor a while, I conflated logging and monitoring. At least, I thought they were two sides of the same coin. I hadn’t considered how uniquely necessary they each were, and how they supported each other.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003ELogging\u003C\u002Fstrong\u003E tells you \u003Cem\u003Ewhat\u003C\u002Fem\u003E happened, and gives you the raw data to track down the issue.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EMonitoring\u003C\u002Fstrong\u003E tells you \u003Cem\u003Ehow\u003C\u002Fem\u003E your application is behaving and can alert you when there are issues.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"cant-have-one-without-the-other\" href=\"#cant-have-one-without-the-other\" class=\"anchor\"\u003E\n  \u003C\u002Fa\u003E\n  Can’t Have One Without the Other\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003ELet’s consider a system that has fantastic logging but no monitoring. It’s obvious why this doesn’t work. No matter how good our logs are, I guarantee that nobody actively reads them — especially when our logs get verbose or use formatting like JSON. It is impractical to assume that someone will comb all those logs and look for errors. Maybe when we have a small set of beta users, we can expect them to report every error so we can go back and look at what happened. But what if we have a million users? We can’t expect every one of those users to report each error they encounter.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--XVH0WIuR--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F4240\u002F0%2AkRs3ZGiGshMrMJYE.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--XVH0WIuR--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F4240\u002F0%2AkRs3ZGiGshMrMJYE.png\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003EThis is where monitoring comes in. We need to put the systems in place that can do the looking up and coordinating for us. We need a system that will let us know when an error happens and, if it is good enough, why that error occurred.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"monitoring\" href=\"#monitoring\" class=\"anchor\"\u003E\n  \u003C\u002Fa\u003E\n  Monitoring\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003ELet’s begin by talking about monitoring goals and what makes a great monitoring system. First, our system must be able to notify us when it detects errors. Second, we should be able to create alerts based on the needs of our system.\u003C\u002Fp\u003E\n\n\u003Cp\u003EWe want to lay out the specific types of events that will determine if our system is performing correctly or not. You may want to be alerted about every error that gets logged. Alternatively, you may be more interested in how fast your system responds in cases. Or, you might be focused on whether your error rates are normal or increasing. You may also be interested in security monitoring and what solution suits your cases. For some additional examples of things to monitor, I’d suggest you check out a great article written by Heroku \u003Ca href=\"https:\u002F\u002Fdevcenter.heroku.com\u002Farticles\u002Flogging-best-practices-guide?preview=1#example-logging-use-cases\"\u003Ehere\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Cp\u003EOne final thing to consider is how our monitoring system can point us toward solutions. This will vary greatly depending on your application; still, it is something to consider when picking your tools.\u003C\u002Fp\u003E\n\n\u003Cp\u003ESpeaking of tools, here are some of my favorite tools to use when I’m monitoring an application. I’m sure there are more specific ones out there. If you’ve got some tools that you really love, then feel free to leave them in the comments!\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EElasticsearch\u003C\u002Fstrong\u003E: This is where I store my logs. It lets me set up monitors and alerts in Grafana based on log messages. With Elasticsearch, I can also do full-text searches when I’m trying to find an error’s cause.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EKibana\u003C\u002Fstrong\u003E: This lets me easily perform live queries against Elasticsearch to assist in debugging.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EGrafana\u003C\u002Fstrong\u003E: Here, I create dashboards that provide high-level overviews of my applications. I also use Grafana for its alerting system.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--4OVAloHr--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F0%2AovRTA5ZglVH4dcmM.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--4OVAloHr--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F0%2AovRTA5ZglVH4dcmM.png\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EInfluxDB\u003C\u002Fstrong\u003E: This time-series database records things like response times, response codes, and any interesting point-in-time data (like success vs error messages within a batch).\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EPushover\u003C\u002Fstrong\u003E: When working as a single engineer in a project, Pushover gives me a simple and cheap notification interface. It directly pushes a notification to my phone whenever an alert is triggered. Grafana also has native support for Pushover, so I only have to put in a few API keys and I am ready to go.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EPagerDuty\u003C\u002Fstrong\u003E: If you are working on a larger project or with a team, then I would suggest \u003Ca href=\"https:\u002F\u002Fwww.pagerduty.com\"\u003EPagerDuty\u003C\u002Fa\u003E. With it, you can schedule specific times when different people (like individuals on your team) receive notifications. You can also create escalation policies in case someone can’t respond quickly enough. Again, Granafa offers native support for PagerDuty.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EHeroku\u003C\u002Fstrong\u003E: There are other monitoring best practices in this \u003Ca href=\"https:\u002F\u002Fdevcenter.heroku.com\u002Farticles\u002Flogging-best-practices-guide?preview=1\"\u003Earticle from Heroku\u003C\u002Fa\u003E. If you are within the Heroku ecosystem, then you can look at their \u003Ca href=\"https:\u002F\u002Felements.heroku.com\u002Faddons#logging\"\u003Elogging add-ons\u003C\u002Fa\u003E (most of which include alerting).\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"monitoring-example-project\" href=\"#monitoring-example-project\" class=\"anchor\"\u003E\n  \u003C\u002Fa\u003E\n  Monitoring Example Project\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003ELet’s look at an example project: a Kubernetes-powered web application behind an NGINX proxy, whose log output and response codes\u002Ftimes we want to monitor. If you aren’t interested in the implementation of these tools, feel free to skip to the next section.\u003C\u002Fp\u003E\n\n\u003Cp\u003EKubernetes automatically writes all logs to stderr and stdout to files on the file system. We can monitor these logs easily, so long as our application correctly writes logs to these streams. As an aside, it is also possible to send your log files directly to Elasticsearch from your application. But for our example project, we want the lowest barrier to entry.\u003C\u002Fp\u003E\n\n\u003Cp\u003ENow that our application is writing logs to the correct locations, let’s set up Elasticsearch, Kibana, and Filebeat to collect the output from the container. Additional and more up-to-date information can be found on the \u003Ca href=\"https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-quickstart.html\"\u003EElastic Cloud Quickstart page\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Cp\u003EFirst, we \u003Ca href=\"https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-eck.html\"\u003Edeploy the Elastic Cloud Operator\u003C\u002Fa\u003E and RBAC rules.\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003Ekubectl apply -f https:\u002F\u002Fdownload.elastic.co\u002Fdownloads\u002Feck\u002F1.3.1\u002Fall-in-one.yaml\n\n# Monitor the output from the operator\nkubectl -n elastic-system logs -f statefulset.apps\u002Felastic-operator\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003ENext, let’s actually \u003Ca href=\"https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-elasticsearch.html\"\u003Edeploy the Elasticsearch cluster\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003Ecat &lt;&lt;EOF | kubectl apply -f -\napiVersion: elasticsearch.k8s.elastic.co\u002Fv1\nkind: Elasticsearch\nmetadata:\n  name: quickstart\nspec:\n  version: 7.10.2\n  nodeSets:\n  - name: default\n    count: 1\n    config:\n      node.store.allow_mmap: false\nEOF\n\n# Wait for the cluster to go green\nkubectl get elasticsearch\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003ENow that we have an Elasticsearch cluster, let’s \u003Ca href=\"https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-kibana.html\"\u003Edeploy Kibana\u003C\u002Fa\u003E so we can visually query Elasticsearch.\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003Ecat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kibana.k8s.elastic.co\u002Fv1\nkind: Kibana\nmetadata:\n  name: quickstart\nspec:\n  version: 7.10.2\n  count: 1\n  elasticsearchRef:\n    name: quickstart\nEOF\n\n# Get information about the kibana deployment\nkubectl get kibana\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003EReview \u003Ca href=\"https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-kibana.html\"\u003Ethis page\u003C\u002Fa\u003E for more information about accessing Kibana.\u003C\u002Fp\u003E\n\n\u003Cp\u003EFinally, we’ll add FileBeat, \u003Ca href=\"https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-beat-quickstart.html\"\u003Eusing this guide\u003C\u002Fa\u003E, to monitor the Kubernetes logs and ship them to Elasticsearch.\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003Ecat &lt;&lt;EOF | kubectl apply -f -\napiVersion: beat.k8s.elastic.co\u002Fv1beta1\nkind: Beat\nmetadata:\n  name: quickstart\nspec:\n  type: filebeat\n  version: 7.10.2\n  elasticsearchRef:\n    name: quickstart\n  config:\n    filebeat.inputs:\n    - type: container\n      paths:\n      - \u002Fvar\u002Flog\u002Fcontainers\u002F*.log\n  daemonSet:\n    podTemplate:\n      spec:\n        dnsPolicy: ClusterFirstWithHostNet\n        hostNetwork: true\n        securityContext:\n          runAsUser: 0\n        containers:\n        - name: filebeat\n          volumeMounts:\n          - name: varlogcontainers\n            mountPath: \u002Fvar\u002Flog\u002Fcontainers\n          - name: varlogpods\n            mountPath: \u002Fvar\u002Flog\u002Fpods\n          - name: varlibdockercontainers\n            mountPath: \u002Fvar\u002Flib\u002Fdocker\u002Fcontainers\n        volumes:\n        - name: varlogcontainers\n          hostPath:\n            path: \u002Fvar\u002Flog\u002Fcontainers\n        - name: varlogpods\n          hostPath:\n            path: \u002Fvar\u002Flog\u002Fpods\n        - name: varlibdockercontainers\n          hostPath:\n            path: \u002Fvar\u002Flib\u002Fdocker\u002Fcontainers\nEOF\n\n# Wait for the beat to go green\nkubectl get beat\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003ESince our application uses NGINX as a proxy, we can use \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Finfluxdata\u002Fnginx-influxdb-module\"\u003Ethis wonderful module\u003C\u002Fa\u003E to write the response codes and times to InfluxDB.\u003C\u002Fp\u003E\n\n\u003Cp\u003ENext, you can follow \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fgrafana\u002Fhelm-charts\u002Fblob\u002Fmain\u002Fcharts\u002Fgrafana\u002FREADME.md\"\u003Ethis guide\u003C\u002Fa\u003E to get Grafana running in your Kubernetes cluster. After that, \u003Ca href=\"https:\u002F\u002Fgrafana.com\u002Fdocs\u002Fgrafana\u002Flatest\u002Fdatasources\u002F\"\u003Eset up the two data sources\u003C\u002Fa\u003E we are using: InfluxDB and Elasticsearch.\u003C\u002Fp\u003E\n\n\u003Cp\u003EFinally, set up whatever \u003Ca href=\"https:\u002F\u002Fgrafana.com\u002Fdocs\u002Fgrafana\u002Flatest\u002Falerting\u002Fnotifications\u002F\"\u003Ealert channel notifiers\u003C\u002Fa\u003E you wish to use. In my case, I’d use Pushover since I’m just one developer. You may be more interested in something like \u003Ca href=\"https:\u002F\u002Fwww.pagerduty.com\u002F\"\u003EPagerDuty\u003C\u002Fa\u003E if you need a fully-featured notification channel.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAnd there you have it! We’ve got an application — one we can set up dashboards and alerts for using Grafana.\u003C\u002Fp\u003E\n\n\u003Cp\u003EThis setup can notify us about all sorts of issues. For example:\u003C\u002Fp\u003E\n\n\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003EWe detected any ERROR level logs.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EWe are receiving too many error response codes from our system.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EWe are noticing our application responding slower than usual.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Cp\u003EWe did all this without making many changes to our application; and yet, we now have a lot of tools available to us. We can now instrument our code to record interesting points in time using InfluxDB. For example, if we received a batch of 500 messages and 39 of them were unable to be parsed, we can post a message to InfluxDB telling us that we received 461 valid messages and 39 invalid messages. We can then set up an alert in Grafana to let us know if that ratio of valid to invalid messages spikes.\u003C\u002Fp\u003E\n\n\u003Cp\u003EEssentially, anything that is interesting to code should be interesting to monitor; now, we have the tools necessary to monitor anything interesting in our application.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAs a small bonus, here is a Pushover alert that I received from a setup similar to the one described above. I accidentally took down my father’s website during an experiment and this was the result.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--SZSg8jZN--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F5200\u002F0%2AmFFZkyHz_xp4uQ79.jpeg\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--SZSg8jZN--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F5200\u002F0%2AmFFZkyHz_xp4uQ79.jpeg\" alt=\"\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003EAt this point, I’ll give you a break to digest everything I’ve talked about. In \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fbloveless\u002Flogging-best-practices-part-2-3916\"\u003Epart two\u003C\u002Fa\u003E I’ll be discussing some logging best practices.\u003C\u002Fp\u003E\n\n";c.body_markdown="## Introduction\n\nWhat do you do when your application is down? Better yet: How can you *predict* when your application may go down? How do you begin an investigation in the most efficient way possible and resolve issues quickly?\n\nUnderstanding the difference between logging and monitoring is critical, and can make all the difference in your ability to trace issues back to their root cause. If you confuse the two or use one without the other, you’re setting yourself up for long nights and weekends debugging your app.\n\nIn this article, we’ll look at how to effectively log and monitor your systems. I’ll tell you about a few good practices that I’ve learned over the years and some interesting metrics that you may want to monitor in your systems. Finally, I’ll show you a small web application that had no monitoring, alerting, or logging. I’ll demonstrate how I fixed the logging and how I’ve implemented monitoring and alerting around those logs.\n\nEveryone has some sort of logging in their applications, even if it’s just writing to a file to review later. By the end of this article, I hope to convince you that logging without monitoring is about as good as no logging at all. Along the way, we can review some best practices for becoming a better logger.\n\n## Logging vs Monitoring\n\nFor a while, I conflated logging and monitoring. At least, I thought they were two sides of the same coin. I hadn’t considered how uniquely necessary they each were, and how they supported each other.\n\n**Logging** tells you *what* happened, and gives you the raw data to track down the issue.\n\n**Monitoring** tells you *how* your application is behaving and can alert you when there are issues.\n\n## Can’t Have One Without the Other\n\nLet’s consider a system that has fantastic logging but no monitoring. It’s obvious why this doesn’t work. No matter how good our logs are, I guarantee that nobody actively reads them — especially when our logs get verbose or use formatting like JSON. It is impractical to assume that someone will comb all those logs and look for errors. Maybe when we have a small set of beta users, we can expect them to report every error so we can go back and look at what happened. But what if we have a million users? We can’t expect every one of those users to report each error they encounter.\n\n![](https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F4240\u002F0*kRs3ZGiGshMrMJYE.png)\n\nThis is where monitoring comes in. We need to put the systems in place that can do the looking up and coordinating for us. We need a system that will let us know when an error happens and, if it is good enough, why that error occurred.\n\n## Monitoring\n\nLet’s begin by talking about monitoring goals and what makes a great monitoring system. First, our system must be able to notify us when it detects errors. Second, we should be able to create alerts based on the needs of our system.\n\nWe want to lay out the specific types of events that will determine if our system is performing correctly or not. You may want to be alerted about every error that gets logged. Alternatively, you may be more interested in how fast your system responds in cases. Or, you might be focused on whether your error rates are normal or increasing. You may also be interested in security monitoring and what solution suits your cases. For some additional examples of things to monitor, I’d suggest you check out a great article written by Heroku [here](https:\u002F\u002Fdevcenter.heroku.com\u002Farticles\u002Flogging-best-practices-guide?preview=1#example-logging-use-cases).\n\nOne final thing to consider is how our monitoring system can point us toward solutions. This will vary greatly depending on your application; still, it is something to consider when picking your tools.\n\nSpeaking of tools, here are some of my favorite tools to use when I’m monitoring an application. I’m sure there are more specific ones out there. If you’ve got some tools that you really love, then feel free to leave them in the comments!\n\n**Elasticsearch**: This is where I store my logs. It lets me set up monitors and alerts in Grafana based on log messages. With Elasticsearch, I can also do full-text searches when I’m trying to find an error’s cause.\n\n**Kibana**: This lets me easily perform live queries against Elasticsearch to assist in debugging.\n\n**Grafana**: Here, I create dashboards that provide high-level overviews of my applications. I also use Grafana for its alerting system.\n\n![](https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F0*ovRTA5ZglVH4dcmM.png)\n\n**InfluxDB**: This time-series database records things like response times, response codes, and any interesting point-in-time data (like success vs error messages within a batch).\n\n**Pushover**: When working as a single engineer in a project, Pushover gives me a simple and cheap notification interface. It directly pushes a notification to my phone whenever an alert is triggered. Grafana also has native support for Pushover, so I only have to put in a few API keys and I am ready to go.\n\n**PagerDuty**: If you are working on a larger project or with a team, then I would suggest [PagerDuty](https:\u002F\u002Fwww.pagerduty.com). With it, you can schedule specific times when different people (like individuals on your team) receive notifications. You can also create escalation policies in case someone can’t respond quickly enough. Again, Granafa offers native support for PagerDuty.\n\n**Heroku**: There are other monitoring best practices in this [article from Heroku](https:\u002F\u002Fdevcenter.heroku.com\u002Farticles\u002Flogging-best-practices-guide?preview=1). If you are within the Heroku ecosystem, then you can look at their [logging add-ons](https:\u002F\u002Felements.heroku.com\u002Faddons#logging) (most of which include alerting).\n\n## Monitoring Example Project\n\nLet’s look at an example project: a Kubernetes-powered web application behind an NGINX proxy, whose log output and response codes\u002Ftimes we want to monitor. If you aren’t interested in the implementation of these tools, feel free to skip to the next section.\n\nKubernetes automatically writes all logs to stderr and stdout to files on the file system. We can monitor these logs easily, so long as our application correctly writes logs to these streams. As an aside, it is also possible to send your log files directly to Elasticsearch from your application. But for our example project, we want the lowest barrier to entry.\n\nNow that our application is writing logs to the correct locations, let’s set up Elasticsearch, Kibana, and Filebeat to collect the output from the container. Additional and more up-to-date information can be found on the [Elastic Cloud Quickstart page](https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-quickstart.html).\n\nFirst, we [deploy the Elastic Cloud Operator](https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-eck.html) and RBAC rules.\n\n    kubectl apply -f https:\u002F\u002Fdownload.elastic.co\u002Fdownloads\u002Feck\u002F1.3.1\u002Fall-in-one.yaml\n\n    # Monitor the output from the operator\n    kubectl -n elastic-system logs -f statefulset.apps\u002Felastic-operator\n\nNext, let’s actually [deploy the Elasticsearch cluster](https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-elasticsearch.html).\n\n    cat \u003C\u003CEOF | kubectl apply -f -\n    apiVersion: elasticsearch.k8s.elastic.co\u002Fv1\n    kind: Elasticsearch\n    metadata:\n      name: quickstart\n    spec:\n      version: 7.10.2\n      nodeSets:\n      - name: default\n        count: 1\n        config:\n          node.store.allow_mmap: false\n    EOF\n\n    # Wait for the cluster to go green\n    kubectl get elasticsearch\n\nNow that we have an Elasticsearch cluster, let’s [deploy Kibana](https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-kibana.html) so we can visually query Elasticsearch.\n\n    cat \u003C\u003CEOF | kubectl apply -f -\n    apiVersion: kibana.k8s.elastic.co\u002Fv1\n    kind: Kibana\n    metadata:\n      name: quickstart\n    spec:\n      version: 7.10.2\n      count: 1\n      elasticsearchRef:\n        name: quickstart\n    EOF\n\n    # Get information about the kibana deployment\n    kubectl get kibana\n\nReview [this page](https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-deploy-kibana.html) for more information about accessing Kibana.\n\nFinally, we’ll add FileBeat, [using this guide](https:\u002F\u002Fwww.elastic.co\u002Fguide\u002Fen\u002Fcloud-on-k8s\u002Fcurrent\u002Fk8s-beat-quickstart.html), to monitor the Kubernetes logs and ship them to Elasticsearch.\n\n    cat \u003C\u003CEOF | kubectl apply -f -\n    apiVersion: beat.k8s.elastic.co\u002Fv1beta1\n    kind: Beat\n    metadata:\n      name: quickstart\n    spec:\n      type: filebeat\n      version: 7.10.2\n      elasticsearchRef:\n        name: quickstart\n      config:\n        filebeat.inputs:\n        - type: container\n          paths:\n          - \u002Fvar\u002Flog\u002Fcontainers\u002F*.log\n      daemonSet:\n        podTemplate:\n          spec:\n            dnsPolicy: ClusterFirstWithHostNet\n            hostNetwork: true\n            securityContext:\n              runAsUser: 0\n            containers:\n            - name: filebeat\n              volumeMounts:\n              - name: varlogcontainers\n                mountPath: \u002Fvar\u002Flog\u002Fcontainers\n              - name: varlogpods\n                mountPath: \u002Fvar\u002Flog\u002Fpods\n              - name: varlibdockercontainers\n                mountPath: \u002Fvar\u002Flib\u002Fdocker\u002Fcontainers\n            volumes:\n            - name: varlogcontainers\n              hostPath:\n                path: \u002Fvar\u002Flog\u002Fcontainers\n            - name: varlogpods\n              hostPath:\n                path: \u002Fvar\u002Flog\u002Fpods\n            - name: varlibdockercontainers\n              hostPath:\n                path: \u002Fvar\u002Flib\u002Fdocker\u002Fcontainers\n    EOF\n\n    # Wait for the beat to go green\n    kubectl get beat\n\nSince our application uses NGINX as a proxy, we can use [this wonderful module](https:\u002F\u002Fgithub.com\u002Finfluxdata\u002Fnginx-influxdb-module) to write the response codes and times to InfluxDB.\n\nNext, you can follow [this guide](https:\u002F\u002Fgithub.com\u002Fgrafana\u002Fhelm-charts\u002Fblob\u002Fmain\u002Fcharts\u002Fgrafana\u002FREADME.md) to get Grafana running in your Kubernetes cluster. After that, [set up the two data sources](https:\u002F\u002Fgrafana.com\u002Fdocs\u002Fgrafana\u002Flatest\u002Fdatasources\u002F) we are using: InfluxDB and Elasticsearch.\n\nFinally, set up whatever [alert channel notifiers](https:\u002F\u002Fgrafana.com\u002Fdocs\u002Fgrafana\u002Flatest\u002Falerting\u002Fnotifications\u002F) you wish to use. In my case, I’d use Pushover since I’m just one developer. You may be more interested in something like [PagerDuty](https:\u002F\u002Fwww.pagerduty.com\u002F) if you need a fully-featured notification channel.\n\nAnd there you have it! We’ve got an application — one we can set up dashboards and alerts for using Grafana.\n\nThis setup can notify us about all sorts of issues. For example:\n\n* We detected any ERROR level logs.\n\n* We are receiving too many error response codes from our system.\n\n* We are noticing our application responding slower than usual.\n\nWe did all this without making many changes to our application; and yet, we now have a lot of tools available to us. We can now instrument our code to record interesting points in time using InfluxDB. For example, if we received a batch of 500 messages and 39 of them were unable to be parsed, we can post a message to InfluxDB telling us that we received 461 valid messages and 39 invalid messages. We can then set up an alert in Grafana to let us know if that ratio of valid to invalid messages spikes.\n\nEssentially, anything that is interesting to code should be interesting to monitor; now, we have the tools necessary to monitor anything interesting in our application.\n\nAs a small bonus, here is a Pushover alert that I received from a setup similar to the one described above. I accidentally took down my father’s website during an experiment and this was the result.\n\n![](https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F5200\u002F0*mFFZkyHz_xp4uQ79.jpeg)\n\nAt this point, I’ll give you a break to digest everything I’ve talked about. In [part two](https:\u002F\u002Fdev.to\u002Fbloveless\u002Flogging-best-practices-part-2-3916) I’ll be discussing some logging best practices.\n";c.user={name:"Brennon Loveless",username:f,twitter_username:a,github_username:f,website_url:a,profile_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--rB8DPKTv--\u002Fc_fill,f_auto,fl_progressive,h_640,q_auto,w_640\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F395336\u002F9608215b-d9b9-41ab-b3d6-963a0b49e308.jpeg",profile_image_90:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--BDp6H4en--\u002Fc_fill,f_auto,fl_progressive,h_90,q_auto,w_90\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F395336\u002F9608215b-d9b9-41ab-b3d6-963a0b49e308.jpeg"};return {data:[{}],fetch:{"data-v-25febe66:0":{article:c}},mutations:[["SET_CURRENT_ARTICLE",c]]}}(null,"2021-02-03T12:56:27Z",{},"https:\u002F\u002Fdev.to\u002Fbloveless\u002Flogging-v-monitoring-part-1-47lk",4,"bloveless")));