window.__NUXT__=(function(a,b,c,d,e,f){return {staticAssetsBase:"\u002F_nuxt\u002Fstatic\u002F1634322943",layout:"default",error:a,state:{currentArticle:{type_of:"article",id:500376,title:"SRE + Honeycomb: Observability for Service Reliability",description:"As a Customer Advocate, I talk to a lot of prospective Honeycomb users who want to understand how obs...",readable_publish_date:"Oct 30 '20",slug:"sre-honeycomb-observability-for-service-reliability-53e3",path:"\u002Fhoneycombio\u002Fsre-honeycomb-observability-for-service-reliability-53e3",url:c,comments_count:0,public_reactions_count:d,collection_id:a,published_timestamp:b,positive_reactions_count:d,cover_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--aeXUT0eX--\u002Fc_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fcu4vmkcr03w6v0mava97.jpg",social_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--Bos9O6X_--\u002Fc_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fcu4vmkcr03w6v0mava97.jpg",canonical_url:c,created_at:"2020-10-28T17:23:14Z",edited_at:a,crossposted_at:a,published_at:b,last_comment_at:b,reading_time_minutes:11,tag_list:"sre, reliability, observability, devops",tags:["sre","reliability","observability","devops"],body_html:"\u003Cp\u003EAs a Customer Advocate, I talk to a lot of prospective Honeycomb users who want to understand how observability fits into their existing Site Reliability Engineering (SRE) practice. While I have enough of a familiarity with the discipline to get myself into trouble, I wanted to learn more about what SREs do in their day-to-day work so that I‚Äôd be better able to help them determine if Honeycomb is a good fit for their needs.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAfter doing some reading on my own, I asked my fellow Bees for their thoughts on various definitions of SRE floating out in the wild. Luckily for me, a couple of teammates who‚Äôve worked as SREs chimed in: Principal Developer Advocate \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Flizthegrey\"\u003ELiz Fong-Jones\u003C\u002Fa\u003E, Developer Advocate \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fshelbyspees\"\u003EShelby Spees\u003C\u002Fa\u003E, and Lead Integrations Engineer \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fpaulosman\"\u003EPaul Osman\u003C\u002Fa\u003E. Liz recommended a \u003Ca href=\"https:\u002F\u002Fcloud.google.com\u002Fblog\u002Fproducts\u002Fgcp\u002Fsre-vs-devops-competing-standards-or-close-friends\"\u003Eblog post\u003C\u002Fa\u003E from her time at Google featuring a video where she and \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fsethvargo\"\u003ESeth Vargo\u003C\u002Fa\u003E explain how SRE and DevOps relate. I definitely recommend giving it a watch:\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ciframe width=\"710\" height=\"399\" src=\"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002FuTEL8Ff1Zvk\" allowfullscreen loading=\"lazy\"\u003E\n\u003C\u002Fiframe\u003E\n\u003C\u002Fp\u003E\n\n\u003Cp\u003EAfter that, Shelby offered to discuss the topic further 1-on-1 and I was happy to take her up on it. For more color, below is the conversation between Shelby &amp; me (two bees in a pod) üêù\u003C\u002Fp\u003E\n\n\u003Ch2\u003EJenni and Shelby discuss SRE and Honeycomb\u003C\u002Fh2\u003E\n\n\u003Cp\u003E\u003Cb\u003E\u003Cem\u003EShelby\u003C\u002Fem\u003E: \u003C\u002Fb\u003EThe Google SRE book establishes how they build reliable systems at Google, but the actual work of SREs has been around for decades. It‚Äôs just that this specific term \"site reliability engineering\" was coined and popularized by Google. SRE is really about helping accomplish business goals by making software systems work better, because \"reliability\" is defined by the end user's experience.\u003C\u002Fp\u003E\n\n\u003Cp\u003EOne of the biggest takeaways for me from the Google book that I hadn‚Äôt seen elsewhere was the introduction of SLOs (service level objectives) and error budget. \u003Cstrong\u003ESREs are the ones who determine: How many engineering brain cycles do we want to sink into this new code? Versus do we spend our time making our customers happier by eliminating tech debt? That‚Äôs kind of the motivation behind SLOs, that decision-making process.\u003C\u002Fstrong\u003E ‚öñÔ∏è\u003C\u002Fp\u003E\n\n\u003Cp\u003EA rough example of an SLO for a Netflix-like streaming company might be: Can your customers watch their TV show with &lt;10 sec of buffering? For the viewer, it doesn‚Äôt matter what part of your system is causing the buffering to happen, they just want to watch the show. So with SLOs, it was a novel idea just be able to measure whether or not people are having a bad experience based on what your systems are reporting.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EBuilding on that, we can start to measure ‚Äúgood enough‚Äù vs. ‚Äútoo broken.‚Äù Issues and incidents are rarely all-or-nothing nowadays.\u003C\u002Fstrong\u003E If our service is degraded, how soon will our customers notice and complain? Refreshing once in a multi-hour streaming session is reasonable, but twice per episode probably isn‚Äôt. So SREs do the work of figuring out how to convert the measurements you have coming out of your system into something that gives you insight into your user experience in real-time.\u003C\u002Fp\u003E\n\n\u003Cp\u003EPerhaps an SLO where 99.9% of streaming sessions have &lt;10s of buffering is fine, people can hit refresh if there are buffering errors, but it‚Äôs not catastrophic. \u003Cstrong\u003ESREs are the ones who measure how much risk are we willing to take to build a new feature, or how degraded can we allow our service to get.\u003C\u002Fstrong\u003E While it‚Äôs not usually as simple as ‚Äúnew features vs. reliability,‚Äù that‚Äôs a good way to understand it when you‚Äôre first learning about this stuff.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAt the end of the day, all of the work we do is a living experiment. SREs have a finger on the pulse to see if this is good enough to make a decision on the trade-off between building new code or improving system reliability. \u003Cstrong\u003E#everythingisanexperiment #honeycombcorevalues\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Yea! And this really reminds me of the time when I was interviewing Erol Blakely, Director of SRE at \u003Ca href=\"https:\u002F\u002Fwww.ecobee.com\u002F\"\u003Eecobee\u003C\u002Fa\u003E for their \u003Ca href=\"https:\u002F\u002Fwww.honeycomb.io\u002Fcase-studies\u002Fbees-working-together-how-s-engineers-adopted-honeycomb\u002F\"\u003Ecustomer case study\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Cp\u003EErol said two things that stuck out to me about their SRE practice: 1) that SLOs are his three favorite letters üòÇ and 2) that before Honeycomb, they had \u003Cstrong\u003Eparalysis by analysis whenever they would try to define their SLOs\u003C\u002Fstrong\u003E, because their methodology and tooling for creating SLIs (service level indicators) and then creating SLOs would take weeks of white-boarding and head-banging ü§ò to try to get the right measurements in place and accurate. He said it also took a lot of time to even see if their engineering objectives were actually something the customer or the business cared about.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAll of this was 10x harder on ecobee to solve for because their tools and dashboards lacked the functionality, making it impossible to iterate on their SLIs and SLOs without more head-banging and yadayadayada. \u003Cstrong\u003ESo when Erol saw Honeycomb‚Äôs rapidly iterative SLO feature, its baked anomaly detector, AND built-in error budgets, he said it was love at first sight\u003C\u002Fstrong\u003E üòç and that he ‚Äú\u003Cem\u003Ewould‚Äôve needed to hire a full-time Grafana and Prometheus admin to do the work that Honeycomb could do for him.\u003C\u002Fem\u003E‚Äù\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Totally right! SLOs help teams better conversations around your engineering effort, but it‚Äôs not always easy to define them. And it really just gives the whole company a high-level view of the system and the purpose of the system. SLOs offer support to the teams who are building these systems on how to make them more resilient and understand the impact of changes.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Teams keep telling me ‚Äú\u003Cem\u003EJenni, we‚Äôre practicing SRE\u003C\u002Fem\u003E‚Äù and I keep seeing ‚ÄúSRE‚Äù titles, so then why do some people still consider it a niche role?\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Here‚Äôs a good explanation from Paul Osman (Lead Integrations Engineer) on SREs:\u003C\u002Fp\u003E\n\n\u003Cblockquote\u003E\n\u003Cem\u003E\"The SRE role varies wildly from company to company. Some organizations have adopted the Google model fairly consistently, in other orgs it's closer to what Facebook call 'Production Engineering'. In some orgs, it's re-branded 'ops engineering', in some orgs it's closer to what some people call 'infrastructure engineering'. And of course, it's everywhere in between.\u003C\u002Fem\u003E\n\n\u003Cem\u003EIn my mind, what defines an SRE vs those other roles is that an SRE team should be working, through systems and processes, to help determine how much time and effort should be spent on reliability and helping the larger eng org move that work forward.\"\u003C\u002Fem\u003E\n\u003C\u002Fblockquote\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EThe amount of people doing SRE according to how Google wrote about it is a small minority of actual SRE teams\u003C\u002Fstrong\u003E. Part of the issue with making decisions based on ‚Äú\u003Cem\u003EWhat‚Äôs the impact to our customer, services, business?\u003C\u002Fem\u003E‚Äù is that you have to actually talk to business stakeholders and convince them to balance feature development with paying down technical debt.\u003C\u002Fp\u003E\n\n\u003Cp\u003ERegardless of whether a team is practicing SRE ‚Äúby the book‚Äù or not, these are difficult conversations to have. It‚Äôs not easy to get buy-in, which is why I appreciate how well SLOs map to business KPIs and product user stories. The ability to translate between business priorities and engineering effort is a core skill for SREs.\u003C\u002Fp\u003E\n\n\u003Cp\u003EIn reality, companies hire SREs and then silo them into ops or devops-y roles without the authority to influence prioritization decisions, so teams have to continue fighting fires indefinitely. You see the worst version of this in SRE job descriptions where they say \"\u003Cem\u003Eengineer CI\u002FCD pipelines\u003C\u002Fem\u003E\" but they mean \"\u003Cem\u003Emanage our Jenkins configurations because we don‚Äôt want the developers ever thinking about builds.\u003C\u002Fem\u003E\"\u003C\u002Fp\u003E\n\n\u003Cp\u003EOther times, a person with lots of experience with ops work gets hired into an SRE role with the opportunity to make a real impact. Since they‚Äôre an ops person, they‚Äôre drawn to interesting infrastructure and platform solutions like Kubernetes, which may indeed solve process problems and help the systems work more reliably at scale. But these solutions also add complexity, so the SRE who set it all up ends up becoming a Kubernetes engineer full-time--now their expertise is needed to keep the system afloat.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003ESRE is such a cross-cutting function, you really need to find people with both the expertise as a software practitioner and the big-picture business mindset.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Obviously a lot easier said than done. Conversations are difficult enough in general, imagine trying to navigate all of those competing interests (and personalities) and how to balance your growth as a company versus making sure your service is reliable. It‚Äôs a crucial tradeoff for the longevity of the business.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Oh yea. But SLOs are a very useful tool for this, especially when they build on the instrumentation that people are already using. Which is why investing in observability can pay dividends in your reliability work.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EWhen you have observability into the business logic of your system (not just system level metrics) AND the high cardinality data on how all your endpoints are behaving or even what individual customers are experiencing, that allows you to go and debug critical production issues quickly because you‚Äôre already collecting rich data on it.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003EThen once you‚Äôve interacted with the data frequently and find yourself asking similar questions over and over, you can then define an SLI based on that query or question you‚Äôre asking. Since you‚Äôve already instrumented your code to send all that context all the time, you don‚Äôt have to do any extra work to add SLI measurements. Then you can use the SLO tool in Honeycomb to show you the compliance with your measurement over time! üòâ\u003C\u002Fp\u003E\n\n\u003Cp\u003EPlus (my favorite part): \u003Cstrong\u003EHoneycomb‚Äôs SLO feature automatically performs \u003Ca href=\"https:\u002F\u002Fdocs.honeycomb.io\u002Fworking-with-your-data\u002Fbubbleup\"\u003EBubbleUp\u003C\u002Fa\u003E over the events considered by your SLI, showing you which fields stand out in the events that fail your SLO.\u003C\u002Fstrong\u003E If you get an alert, you can look at the dimensions in SLO BubbleUp and that gives you a really good starting point for your investigation. So now you‚Äôve tied together business impact (via your SLO) with the developer tooling you already use to observe your systems in production.\u003C\u002Fp\u003E\n\n\u003Cp\u003EYou should check out this \u003Ca href=\"https:\u002F\u002Fstatus.honeycomb.io\u002Fincidents\u002Fjls2lggldlfj\"\u003Eincident report\u003C\u002Fa\u003E from July. \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fmartin308\"\u003EMartin\u003C\u002Fa\u003E was on-call, and this was the first middle of the night page we‚Äôd had in a while. It was an SLO burn alert, so Martin opened up the SLO that was alerting. BubbleUp showed him that the bad events were in a specific availability zone, so he said ‚Äú\u003Cem\u003EOK, I‚Äôll just remove that availability zone from the autoscaling group.\u003C\u002Fem\u003E‚Äù He got a read on the situation in less than five minutes and started working on a fix. üí™\u003C\u002Fp\u003E\n\n\u003Cp\u003EAnd so that‚Äôs why Liz encouraged us to adopt SLOs as a feature: observability makes investigating issues a lot easier. This means that \u003Cstrong\u003Eyou're not just measuring what's important to the business, you're also empowering your team to actually have a positive impact on service reliability, both during incidents and in day-to-day development work.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: So, can you get the same SLO result without instrumenting your code?\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: It depends on the questions you‚Äôre asking. The observability approach to SLOs kinda requires structured data. Most people don‚Äôt have the quality of data to have that level of granularity or insight into the user experience. Instead, people will write their SLO like, ‚Äúwe should have 99.9% uptime,‚Äù although I think the language is usually more declarative than that. But that‚Äôs great! The important thing is to start measuring and learning. Many teams don‚Äôt have the bandwidth to sit down and ask, ‚Äú\u003Cem\u003Ehow many nines of uptime did we have last year?\u003C\u002Fem\u003E‚Äù and they'll likely get a different answer from each monitoring tool they check.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003ESo I would tell people: start with something, and if it turns out you‚Äôre up more than what you defined for your SLO then maybe you have the error budget for taking more risks!\u003C\u002Fstrong\u003E You could do a chaos experiment to learn more about your system or test out some performance enhancements you‚Äôve been wanting to try. But, if you don‚Äôt meet your SLO and find out that it was way too ambitious as a goal, you still learned something. You can adjust the SLO and reset your error budget. Maybe your standards are too high, or you may need to do some reliability work to meet your goal.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAlso, it‚Äôs important to remember that SLOs are an internal tool, to help teams to have better conversations about service reliability. If your hypothesis was incorrect or your goal was too ambitious, that‚Äôs not a bad thing as long as you're learning. It‚Äôs only a problem if you fail to adjust your approach.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: It almost sounds like the learning is more important than meeting the SLO itself.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: I mean, it kind of is. Some teams have so little insight into how their systems run in production that paying attention to any of this stuff will make a huge difference. You don‚Äôt need to be at Google scale to benefit.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EIf you don‚Äôt have good observability, then start with something simple like Pingdom checks: Is it even up??\u003C\u002Fstrong\u003E Look at the history if you have it, set a goal, and then start mapping your reliability efforts to your SLO results. That mapping part is hard, though, which is where observability comes into play. You can gradually add richer instrumentation either through agents on your system or preferably through instrumenting your code. That allows you to start asking more sophisticated questions and creating SLIs that map closer to user experience.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: So SLOs aren‚Äôt like a contract, they‚Äôre an internal tool. That makes it a lot less intimidating to start experimenting with different kinds of instrumentation and SLIs.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: \u003Cstrong\u003EThe goal is to learn how your systems work in production, and instrumenting for observability lets you have more informed SLIs\u003C\u002Fstrong\u003E. Also, you‚Äôre not gonna get it right the first time, it‚Äôs an iterative process. And it doesn‚Äôt have to be a lot of work up-front! Leaning on your observability tooling and instrumentation means you don‚Äôt have to do a bunch of whiteboard work to engineer your own SLO system and host it all yourself.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: So you mentioned chaos engineering. Tbh I‚Äôm not totally familiar with how it relates.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Oh yea! Chaos engineering is a growing field for SREs that builds on the important work of testers and QA. What makes a great tester is that they break a program in novel ways that the developer never expected, which helps the developer write better code or build a better UI.\u003C\u002Fp\u003E\n\n\u003Cp\u003EIt‚Äôs similar with chaos engineering, except now we‚Äôre talking about your services and infrastructure. Chaos experiments involve purposely breaking things in prod üí• to see how bad it can get without your customers noticing. And similar to traditional testing, it‚Äôs not just about breaking things, it‚Äôs about learning. With chaos engineering, the goal is to learn the boundaries of your services. And (as you might expect) your learning is facilitated by having really good observability!\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: I see! SRE involves a lot of experimentation. ü§î\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: I mean, one common thread among the SREs I know is that they really care about their work. And we‚Äôre starting to see that many SREs are interested in Honeycomb. So Jenni, when you‚Äôre talking to them, it‚Äôs all about helping them see how observability supports their existing reliability work.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAt the end of the day (and this blog post) \u003Cstrong\u003ESREs are in a position to make things better for developers and for the business. That feedback loop in production, a shared sense of building vs. fixing, and the work of automation goes a long way to helping build more resilient and reliable systems.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: And the beat goes on. Hey Shelby, we should do this more often.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Yea this was fun. I‚Äôm gonna call you the next time I feel a rant coming on üòÇ\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Well, you got my number. 867-5309.\u003C\u002Fp\u003E\n\n\u003Ch2\u003EDrop me a line!\u003C\u002Fh2\u003E\n\n\u003Cp\u003EIf you‚Äôre an SRE interested in observability to support your reliability work, I want to help you achieve it! Send me a note: \u003Ca href=\"mailto:jenni@honeycomb.io\"\u003E\u003C\u002Fa\u003E\u003Ca href=\"mailto:jenni@honeycomb.io\"\u003Ejenni@honeycomb.io\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003EExperience what Honeycomb can do for your business. Check this \u003Ca href=\"https:\u002F\u002Fwww.honeycomb.io\u002Fget-a-demo?&amp;utm_source=Devto&amp;utm_Devto=blog&amp;utm_campaign=referral&amp;utm_keyword=%7Bkeyword%7D&amp;utm_content=sre-honeycomb-observability-for-service-reliability\"\u003Eshort and sweet demo\u003C\u002Fa\u003E!\u003C\u002Fp\u003E\n\n",body_markdown:"As a Customer Advocate, I talk to a lot of prospective Honeycomb users who want to understand how observability fits into their existing Site Reliability Engineering (SRE) practice. While I have enough of a familiarity with the discipline to get myself into trouble, I wanted to learn more about what SREs do in their day-to-day work so that I‚Äôd be better able to help them determine if Honeycomb is a good fit for their needs.\n\nAfter doing some reading on my own, I asked my fellow Bees for their thoughts on various definitions of SRE floating out in the wild. Luckily for me, a couple of teammates who‚Äôve worked as SREs chimed in: Principal Developer Advocate \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Flizthegrey\"\u003ELiz Fong-Jones\u003C\u002Fa\u003E, Developer Advocate \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fshelbyspees\"\u003EShelby Spees\u003C\u002Fa\u003E, and Lead Integrations Engineer \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fpaulosman\"\u003EPaul Osman\u003C\u002Fa\u003E. Liz recommended a \u003Ca href=\"https:\u002F\u002Fcloud.google.com\u002Fblog\u002Fproducts\u002Fgcp\u002Fsre-vs-devops-competing-standards-or-close-friends\"\u003Eblog post\u003C\u002Fa\u003E from her time at Google featuring a video where she and \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fsethvargo\"\u003ESeth Vargo\u003C\u002Fa\u003E explain how SRE and DevOps relate. I definitely recommend giving it a watch:\n\n{% youtube uTEL8Ff1Zvk %}\n\nAfter that, Shelby offered to discuss the topic further 1-on-1 and I was happy to take her up on it. For more color, below is the conversation between Shelby &amp; me (two bees in a pod) üêù\n\u003Ch2\u003EJenni and Shelby discuss SRE and Honeycomb\u003C\u002Fh2\u003E\n\u003Cb\u003E\u003Cem\u003EShelby\u003C\u002Fem\u003E: \u003C\u002Fb\u003EThe Google SRE book establishes how they build reliable systems at Google, but the actual work of SREs has been around for decades. It‚Äôs just that this specific term \"site reliability engineering\" was coined and popularized by Google. SRE is really about helping accomplish business goals by making software systems work better, because \"reliability\" is defined by the end user's experience.\n\nOne of the biggest takeaways for me from the Google book that I hadn‚Äôt seen elsewhere was the introduction of SLOs (service level objectives) and error budget. \u003Cstrong\u003ESREs are the ones who determine: How many engineering brain cycles do we want to sink into this new code? Versus do we spend our time making our customers happier by eliminating tech debt? That‚Äôs kind of the motivation behind SLOs, that decision-making process.\u003C\u002Fstrong\u003E ‚öñÔ∏è\n\nA rough example of an SLO for a Netflix-like streaming company might be: Can your customers watch their TV show with &lt;10 sec of buffering? For the viewer, it doesn‚Äôt matter what part of your system is causing the buffering to happen, they just want to watch the show. So with SLOs, it was a novel idea just be able to measure whether or not people are having a bad experience based on what your systems are reporting.\n\n\u003Cstrong\u003EBuilding on that, we can start to measure ‚Äúgood enough‚Äù vs. ‚Äútoo broken.‚Äù Issues and incidents are rarely all-or-nothing nowadays.\u003C\u002Fstrong\u003E If our service is degraded, how soon will our customers notice and complain? Refreshing once in a multi-hour streaming session is reasonable, but twice per episode probably isn‚Äôt. So SREs do the work of figuring out how to convert the measurements you have coming out of your system into something that gives you insight into your user experience in real-time.\n\nPerhaps an SLO where 99.9% of streaming sessions have &lt;10s of buffering is fine, people can hit refresh if there are buffering errors, but it‚Äôs not catastrophic. \u003Cstrong\u003ESREs are the ones who measure how much risk are we willing to take to build a new feature, or how degraded can we allow our service to get.\u003C\u002Fstrong\u003E While it‚Äôs not usually as simple as ‚Äúnew features vs. reliability,‚Äù that‚Äôs a good way to understand it when you‚Äôre first learning about this stuff.\n\nAt the end of the day, all of the work we do is a living experiment. SREs have a finger on the pulse to see if this is good enough to make a decision on the trade-off between building new code or improving system reliability. \u003Cstrong\u003E#everythingisanexperiment #honeycombcorevalues\u003C\u002Fstrong\u003E\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Yea! And this really reminds me of the time when I was interviewing Erol Blakely, Director of SRE at \u003Ca href=\"https:\u002F\u002Fwww.ecobee.com\u002F\"\u003Eecobee\u003C\u002Fa\u003E for their \u003Ca href=\"https:\u002F\u002Fwww.honeycomb.io\u002Fcase-studies\u002Fbees-working-together-how-s-engineers-adopted-honeycomb\u002F\"\u003Ecustomer case study\u003C\u002Fa\u003E.\n\nErol said two things that stuck out to me about their SRE practice: 1) that SLOs are his three favorite letters üòÇ and 2) that before Honeycomb, they had \u003Cstrong\u003Eparalysis by analysis whenever they would try to define their SLOs\u003C\u002Fstrong\u003E, because their methodology and tooling for creating SLIs (service level indicators) and then creating SLOs would take weeks of white-boarding and head-banging ü§ò to try to get the right measurements in place and accurate. He said it also took a lot of time to even see if their engineering objectives were actually something the customer or the business cared about.\n\nAll of this was 10x harder on ecobee to solve for because their tools and dashboards lacked the functionality, making it impossible to iterate on their SLIs and SLOs without more head-banging and yadayadayada. \u003Cstrong\u003ESo when Erol saw Honeycomb‚Äôs rapidly iterative SLO feature, its baked anomaly detector, AND built-in error budgets, he said it was love at first sight\u003C\u002Fstrong\u003E üòç and that he ‚Äú\u003Cem\u003Ewould‚Äôve needed to hire a full-time Grafana and Prometheus admin to do the work that Honeycomb could do for him.\u003C\u002Fem\u003E‚Äù\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Totally right! SLOs help teams better conversations around your engineering effort, but it‚Äôs not always easy to define them. And it really just gives the whole company a high-level view of the system and the purpose of the system. SLOs offer support to the teams who are building these systems on how to make them more resilient and understand the impact of changes.\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Teams keep telling me ‚Äú\u003Cem\u003EJenni, we‚Äôre practicing SRE\u003C\u002Fem\u003E‚Äù and I keep seeing ‚ÄúSRE‚Äù titles, so then why do some people still consider it a niche role?\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Here‚Äôs a good explanation from Paul Osman (Lead Integrations Engineer) on SREs:\n\u003Cblockquote\u003E\u003Cem\u003E\"The SRE role varies wildly from company to company. Some organizations have adopted the Google model fairly consistently, in other orgs it's closer to what Facebook call 'Production Engineering'. In some orgs, it's re-branded 'ops engineering', in some orgs it's closer to what some people call 'infrastructure engineering'. And of course, it's everywhere in between.\u003C\u002Fem\u003E\n\n\u003Cem\u003EIn my mind, what defines an SRE vs those other roles is that an SRE team should be working, through systems and processes, to help determine how much time and effort should be spent on reliability and helping the larger eng org move that work forward.\"\u003C\u002Fem\u003E\u003C\u002Fblockquote\u003E\n\u003Cstrong\u003EThe amount of people doing SRE according to how Google wrote about it is a small minority of actual SRE teams\u003C\u002Fstrong\u003E. Part of the issue with making decisions based on ‚Äú\u003Cem\u003EWhat‚Äôs the impact to our customer, services, business?\u003C\u002Fem\u003E‚Äù is that you have to actually talk to business stakeholders and convince them to balance feature development with paying down technical debt.\n\nRegardless of whether a team is practicing SRE ‚Äúby the book‚Äù or not, these are difficult conversations to have. It‚Äôs not easy to get buy-in, which is why I appreciate how well SLOs map to business KPIs and product user stories. The ability to translate between business priorities and engineering effort is a core skill for SREs.\n\nIn reality, companies hire SREs and then silo them into ops or devops-y roles without the authority to influence prioritization decisions, so teams have to continue fighting fires indefinitely. You see the worst version of this in SRE job descriptions where they say \"\u003Cem\u003Eengineer CI\u002FCD pipelines\u003C\u002Fem\u003E\" but they mean \"\u003Cem\u003Emanage our Jenkins configurations because we don‚Äôt want the developers ever thinking about builds.\u003C\u002Fem\u003E\"\n\nOther times, a person with lots of experience with ops work gets hired into an SRE role with the opportunity to make a real impact. Since they‚Äôre an ops person, they‚Äôre drawn to interesting infrastructure and platform solutions like Kubernetes, which may indeed solve process problems and help the systems work more reliably at scale. But these solutions also add complexity, so the SRE who set it all up ends up becoming a Kubernetes engineer full-time--now their expertise is needed to keep the system afloat.\n\n\u003Cstrong\u003ESRE is such a cross-cutting function, you really need to find people with both the expertise as a software practitioner and the big-picture business mindset.\u003C\u002Fstrong\u003E\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Obviously a lot easier said than done. Conversations are difficult enough in general, imagine trying to navigate all of those competing interests (and personalities) and how to balance your growth as a company versus making sure your service is reliable. It‚Äôs a crucial tradeoff for the longevity of the business.\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Oh yea. But SLOs are a very useful tool for this, especially when they build on the instrumentation that people are already using. Which is why investing in observability can pay dividends in your reliability work.\n\n\u003Cstrong\u003EWhen you have observability into the business logic of your system (not just system level metrics) AND the high cardinality data on how all your endpoints are behaving or even what individual customers are experiencing, that allows you to go and debug critical production issues quickly because you‚Äôre already collecting rich data on it.\u003C\u002Fstrong\u003E\n\nThen once you‚Äôve interacted with the data frequently and find yourself asking similar questions over and over, you can then define an SLI based on that query or question you‚Äôre asking. Since you‚Äôve already instrumented your code to send all that context all the time, you don‚Äôt have to do any extra work to add SLI measurements. Then you can use the SLO tool in Honeycomb to show you the compliance with your measurement over time! üòâ\n\nPlus (my favorite part): \u003Cstrong\u003EHoneycomb‚Äôs SLO feature automatically performs \u003Ca href=\"https:\u002F\u002Fdocs.honeycomb.io\u002Fworking-with-your-data\u002Fbubbleup\"\u003EBubbleUp\u003C\u002Fa\u003E over the events considered by your SLI, showing you which fields stand out in the events that fail your SLO.\u003C\u002Fstrong\u003E If you get an alert, you can look at the dimensions in SLO BubbleUp and that gives you a really good starting point for your investigation. So now you‚Äôve tied together business impact (via your SLO) with the developer tooling you already use to observe your systems in production.\n\nYou should check out this \u003Ca href=\"https:\u002F\u002Fstatus.honeycomb.io\u002Fincidents\u002Fjls2lggldlfj\"\u003Eincident report\u003C\u002Fa\u003E from July. \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fmartin308\"\u003EMartin\u003C\u002Fa\u003E was on-call, and this was the first middle of the night page we‚Äôd had in a while. It was an SLO burn alert, so Martin opened up the SLO that was alerting. BubbleUp showed him that the bad events were in a specific availability zone, so he said ‚Äú\u003Cem\u003EOK, I‚Äôll just remove that availability zone from the autoscaling group.\u003C\u002Fem\u003E‚Äù He got a read on the situation in less than five minutes and started working on a fix. üí™\n\nAnd so that‚Äôs why Liz encouraged us to adopt SLOs as a feature: observability makes investigating issues a lot easier. This means that \u003Cstrong\u003Eyou're not just measuring what's important to the business, you're also empowering your team to actually have a positive impact on service reliability, both during incidents and in day-to-day development work.\u003C\u002Fstrong\u003E\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: So, can you get the same SLO result without instrumenting your code?\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: It depends on the questions you‚Äôre asking. The observability approach to SLOs kinda requires structured data. Most people don‚Äôt have the quality of data to have that level of granularity or insight into the user experience. Instead, people will write their SLO like, ‚Äúwe should have 99.9% uptime,‚Äù although I think the language is usually more declarative than that. But that‚Äôs great! The important thing is to start measuring and learning. Many teams don‚Äôt have the bandwidth to sit down and ask, ‚Äú\u003Cem\u003Ehow many nines of uptime did we have last year?\u003C\u002Fem\u003E‚Äù and they'll likely get a different answer from each monitoring tool they check.\n\n\u003Cstrong\u003ESo I would tell people: start with something, and if it turns out you‚Äôre up more than what you defined for your SLO then maybe you have the error budget for taking more risks!\u003C\u002Fstrong\u003E You could do a chaos experiment to learn more about your system or test out some performance enhancements you‚Äôve been wanting to try. But, if you don‚Äôt meet your SLO and find out that it was way too ambitious as a goal, you still learned something. You can adjust the SLO and reset your error budget. Maybe your standards are too high, or you may need to do some reliability work to meet your goal.\n\nAlso, it‚Äôs important to remember that SLOs are an internal tool, to help teams to have better conversations about service reliability. If your hypothesis was incorrect or your goal was too ambitious, that‚Äôs not a bad thing as long as you're learning. It‚Äôs only a problem if you fail to adjust your approach.\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: It almost sounds like the learning is more important than meeting the SLO itself.\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: I mean, it kind of is. Some teams have so little insight into how their systems run in production that paying attention to any of this stuff will make a huge difference. You don‚Äôt need to be at Google scale to benefit.\n\n\u003Cstrong\u003EIf you don‚Äôt have good observability, then start with something simple like Pingdom checks: Is it even up??\u003C\u002Fstrong\u003E Look at the history if you have it, set a goal, and then start mapping your reliability efforts to your SLO results. That mapping part is hard, though, which is where observability comes into play. You can gradually add richer instrumentation either through agents on your system or preferably through instrumenting your code. That allows you to start asking more sophisticated questions and creating SLIs that map closer to user experience.\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: So SLOs aren‚Äôt like a contract, they‚Äôre an internal tool. That makes it a lot less intimidating to start experimenting with different kinds of instrumentation and SLIs.\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: \u003Cstrong\u003EThe goal is to learn how your systems work in production, and instrumenting for observability lets you have more informed SLIs\u003C\u002Fstrong\u003E. Also, you‚Äôre not gonna get it right the first time, it‚Äôs an iterative process. And it doesn‚Äôt have to be a lot of work up-front! Leaning on your observability tooling and instrumentation means you don‚Äôt have to do a bunch of whiteboard work to engineer your own SLO system and host it all yourself.\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: So you mentioned chaos engineering. Tbh I‚Äôm not totally familiar with how it relates.\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Oh yea! Chaos engineering is a growing field for SREs that builds on the important work of testers and QA. What makes a great tester is that they break a program in novel ways that the developer never expected, which helps the developer write better code or build a better UI.\n\nIt‚Äôs similar with chaos engineering, except now we‚Äôre talking about your services and infrastructure. Chaos experiments involve purposely breaking things in prod üí• to see how bad it can get without your customers noticing. And similar to traditional testing, it‚Äôs not just about breaking things, it‚Äôs about learning. With chaos engineering, the goal is to learn the boundaries of your services. And (as you might expect) your learning is facilitated by having really good observability!\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: I see! SRE involves a lot of experimentation. ü§î\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: I mean, one common thread among the SREs I know is that they really care about their work. And we‚Äôre starting to see that many SREs are interested in Honeycomb. So Jenni, when you‚Äôre talking to them, it‚Äôs all about helping them see how observability supports their existing reliability work.\n\nAt the end of the day (and this blog post) \u003Cstrong\u003ESREs are in a position to make things better for developers and for the business. That feedback loop in production, a shared sense of building vs. fixing, and the work of automation goes a long way to helping build more resilient and reliable systems.\u003C\u002Fstrong\u003E\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: And the beat goes on. Hey Shelby, we should do this more often.\n\n\u003Cem\u003E\u003Cb\u003EShelby\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Yea this was fun. I‚Äôm gonna call you the next time I feel a rant coming on üòÇ\n\n\u003Cem\u003E\u003Cb\u003EJenni\u003C\u002Fb\u003E\u003C\u002Fem\u003E: Well, you got my number. 867-5309.\n\u003Ch2\u003EDrop me a line!\u003C\u002Fh2\u003E\nIf you‚Äôre an SRE interested in observability to support your reliability work, I want to help you achieve it! Send me a note: \u003Ca href=\"mailto:jenni@honeycomb.io\"\u003Ejenni@honeycomb.io\u003C\u002Fa\u003E\n\nExperience what Honeycomb can do for your business. Check this [short and sweet demo] (https:\u002F\u002Fwww.honeycomb.io\u002Fget-a-demo?&utm_source=Devto&utm_Devto=blog&utm_campaign=referral&utm_keyword=%7Bkeyword%7D&utm_content=sre-honeycomb-observability-for-service-reliability)!",user:{name:"Jenni B",username:e,twitter_username:e,github_username:a,website_url:a,profile_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--rDNCNn3B--\u002Fc_fill,f_auto,fl_progressive,h_640,q_auto,w_640\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F499616\u002F83a4232b-6ea8-421f-9d09-9ae4ce79bd44.jpg",profile_image_90:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--Na0iqZ8W--\u002Fc_fill,f_auto,fl_progressive,h_90,q_auto,w_90\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F499616\u002F83a4232b-6ea8-421f-9d09-9ae4ce79bd44.jpg"},organization:{name:"Honeycomb.io",username:f,slug:f,profile_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--LiV-WIYh--\u002Fc_fill,f_auto,fl_progressive,h_640,q_auto,w_640\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Forganization\u002Fprofile_image\u002F2037\u002F81a225aa-09c0-48ce-9977-b7308f2119a1.jpg",profile_image_90:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--O-p0wsWZ--\u002Fc_fill,f_auto,fl_progressive,h_90,q_auto,w_90\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Forganization\u002Fprofile_image\u002F2037\u002F81a225aa-09c0-48ce-9977-b7308f2119a1.jpg"}}},serverRendered:true,routePath:"\u002Farticles\u002Fjennitbd\u002F500376",config:{_app:{basePath:"\u002F",assetsPath:"\u002F_nuxt\u002F",cdnURL:a}}}}(null,"2020-10-30T16:19:12Z","https:\u002F\u002Fdev.to\u002Fhoneycombio\u002Fsre-honeycomb-observability-for-service-reliability-53e3",10,"jennitbd","honeycombio"));