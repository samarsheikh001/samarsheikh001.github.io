__NUXT_JSONP__("/articles/ksatchit/523140", (function(a,b,c,d,e,f){c.type_of="article";c.id=523140;c.title="LitmusChaos: A Reflection On The Past Six Months";c.description="This blog’s title suggests that it may have arrived a month early. After all, reflections are mostly...";c.readable_publish_date="Nov 23 '20";c.slug="litmuschaos-a-reflection-on-the-past-six-months-d6a";c.path="\u002Fksatchit\u002Flitmuschaos-a-reflection-on-the-past-six-months-d6a";c.url=d;c.comments_count=0;c.public_reactions_count=e;c.collection_id=a;c.published_timestamp=b;c.positive_reactions_count=e;c.cover_image="https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--5pjZlwwb--\u002Fc_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fae1uv5sgbuhia55slz2r.png";c.social_image="https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--o3A_fgOD--\u002Fc_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fae1uv5sgbuhia55slz2r.png";c.canonical_url=d;c.created_at="2020-11-23T17:39:44Z";c.edited_at="2020-11-30T04:06:43Z";c.crossposted_at=a;c.published_at=b;c.last_comment_at=b;c.reading_time_minutes=15;c.tag_list="kubernetes, devops, sre, litmuschaos";c.tags=["kubernetes","devops","sre","litmuschaos"];c.body_html="\u003Cp\u003EThis blog’s title suggests that it may have arrived a month early. After all, reflections are mostly written at year-end with predictions and goals for the new year. But to those of us accustomed to compartmentalizing our lives around the festival called “KubeCon”, this doesn’t seem out of place. Also, the choice of six months is thanks to the fact that we have been a CNCF sandbox project for (nearly) this duration. While some of us have been lucky to witness first-hand the ongoings in the project from a vantage point as maintainers, we felt the need for us to share them with the larger litmus community. So, this article is going to be a mix of updates as well as opinions\u002Ftrends we are observing in the cloud-native chaos engineering space, with generous references to specific docs\u002Fblog posts\u002Fvideos highlighting them. \u003C\u002Fp\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"the-project-has-grown\" href=\"#the-project-has-grown\"\u003E\n  \u003C\u002Fa\u003E\n  The Project Has Grown...\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003E...And how! Thanks in no small part to being accepted as a CNCF sandbox project. While we were convinced of the need for a cloud-native chaos engineering solution (which is what motivated us to start Litmus in the first place), the reach and vibrant nature of the CNCF developer &amp; end-user community amplified the interest in the project leading to increased contributions from developers across different organizations (including RedHat, Intuit, ContainerSolutions, Microsoft, etc.,) as well as new adopters (Okteto, WeScale, NetApp). It also paved the way for greater collaboration &amp; integrations with other projects on the CNCF landscape that are focused on solving different kinds of challenges around application delivery (for ex: Okteto, Argo, Keptn, Spinnaker), about which we shall delve a bit more in subsequent sections. More importantly, it has helped generate more dialogue with diverse folks who are at various stages of the cloud-native journey - right from those that have crossed the chasm &amp; have turned into mature\u002Fwell-oiled adopters of the paradigm to those that are in the process of re-architecting their applications into microservices and migrating them onto their first Kubernetes staging clusters. The Litmus slack channel has never been more active (it saw a 70%+ increase in members), and we are also grateful for all those on-demand zoom calls and pages-worth slack conversations - it would be an understatement to say that more than 90% of the features\u002Fenhancements and fixes that went in during this period were direct end-results of this dialogue. Talk about being community-driven!! \u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--39pV39JF--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002F3do1hmvjxyaqcp13ad4c.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--39pV39JF--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002F3do1hmvjxyaqcp13ad4c.png\" alt=\"Alt Text\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"evolution-of-cloudnative-chaos-engineering\" href=\"#evolution-of-cloudnative-chaos-engineering\"\u003E\n  \u003C\u002Fa\u003E\n  Evolution Of (Cloud-Native) Chaos Engineering\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EWhile the principles around which we helped define the \u003Ca href=\"https:\u002F\u002Fwww.cncf.io\u002Fblog\u002F2019\u002F11\u002F06\u002Fcloud-native-chaos-engineering-enhancing-kubernetes-application-resiliency\u002F\"\u003E\u003Cem\u003Esub-category\u003C\u002Fem\u003E\u003C\u002Fa\u003E of cloud-native chaos engineering (a little over a year ago) continue to hold true (validated by the emergence of other projects sharing similar values), we have noticed some developments in this space since. \u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003EThese are also things that influenced us as we developed and iterated on the litmus framework, resulting in several new features &amp; integrations.\u003C\u002Fem\u003E \u003C\u002Fp\u003E\n\n\u003Cp\u003EProminent amongst those has been the call for “left-shift” of chaos - which is now being increasingly viewed as part of the delivery pipelines as against something run only in production environments, by SRE or Ops persona. This is definitely an interesting departure from traditionally held views about CE. While the “exploratory” model of chaos engineering with controlled failures on prod &amp; well-defined hypotheses in a gameday setting is still the Nirvana of a mature devops practice, we found an urgent need amongst the community for doing it much earlier in an automated way (read CI\u002FCD pipelines) in the wake of overhauls to application architecture (making it microservices oriented) &amp; their impending\u002Fongoing migration to Kubernetes.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cem\u003ENote that this is not to say chaos engineering doesn’t apply to monolithic apps or non-Kubernetes environments, just that the recent shift to a cloud-native mindset has brought about new practices.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n\n\n\u003Cblockquote\u003E\n\u003Cp\u003EWhere do you use or intend to use Chaos Engineering in your Kubernetes architecture?\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fchaosengineering?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#chaosengineering\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FKubernetes?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Kubernetes\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fcncf?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#cncf\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDevOps?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#DevOps\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk8s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k8s\u003C\u002Fa\u003E\u003C\u002Fp\u003E— LitmusChaos (Chaos Engineering for Kubernetes) (\u003Ca class=\"mentioned-user\" href=\"https:\u002F\u002Fdev.to\u002Flitmuschaos\"\u003E@litmuschaos\u003C\u002Fa\u003E\n) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FLitmusChaos\u002Fstatus\u002F1274978096106532864?ref_src=twsrc%5Etfw\"\u003EJune 22, 2020\u003C\u002Fa\u003E\n\u003C\u002Fblockquote\u003E \n\n\u003Cp\u003EA \u003Ca href=\"https:\u002F\u002Fyoutu.be\u002Fs-8HwXMdeMk?t=786\"\u003Epanel discussion\u003C\u002Fa\u003E we curated recently to dig more perspective around this topic and other trends in this space was especially enlightening. So have been the conversations with different users (the persona ranging from application developers to people classifying themselves as “devops engineers” focused on delivery tooling, QA architects, SREs, VPs &amp; CTOs). Here is a summarization: \u003C\u002Fp\u003E\n\n\u003Cul\u003E\n\u003Cli\u003EReliability verification is needed right from the earliest stages of application\u002Fplatform development. Something that is being termed as the “Chaos First” principle \u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\n\u003Cblockquote\u003E\n\u003Cp\u003EChaos first is a really important principle. It’s too hard to add resilience to a finished project. Needs to be a gate for deployment in the first place. \u003Ca href=\"https:\u002F\u002Ft.co\u002Fxi98y8JZpJ\"\u003E\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Ft.co\u002Fxi98y8JZpJ\"\u003Ehttps:\u002F\u002Ft.co\u002Fxi98y8JZpJ\u003C\u002Fa\u003E\u003C\u002Fp\u003E— adrian cockcroft (\u003Ca class=\"mentioned-user\" href=\"https:\u002F\u002Fdev.to\u002Fadrianco\"\u003E@adrianco\u003C\u002Fa\u003E\n) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fadrianco\u002Fstatus\u002F1201703004014907392?ref_src=twsrc%5Etfw\"\u003EDecember 3, 2019\u003C\u002Fa\u003E\n\u003C\u002Fblockquote\u003E \n\n\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003EChaos to “test” or unearth issues in the observability frameworks is being increasingly seen as a way to lower the entry-barrier for chaos engineering in many organizations that are generally apprehensive about the idea. Litmus has seen adoption in multiple organizations with this use-case. In some cases, this is the main motivation too (for ex: testing log-based anomaly detection in \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fblob\u002Fmaster\u002Fadopters\u002Forganizations\u002Fzebrium.md#how-do-we-use-litmus\"\u003EZebrium\u003C\u002Fa\u003E!)\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EThere is a definite need identified to bring in methods to define steady-state of systems &amp; hypothesize about the impact of chaos in a declarative way, without which the advantage of a Kubernetes-native\u002FYAML based approach is nullified. This is a prerequisite for automated SLO validation.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EOrganic integrations emerge between cloud-native chaos toolsets &amp; observability frameworks, with native support for chaos events, metrics (and other elements). Observability was &amp; continues to be a prerequisite for chaos engineering. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EThe “run-everywhere” drive in chaos engineering has brought it to the developer’s doorstep and has become a much-used strategy in dev tests. This, combined with the self-service model where developers are provided “namespaces” with “quotas”, has resulted in the need for chaos frameworks to be “multi-tenant” aware, with implications right from RBACs needed for experimentation to the resource consumption at scale.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\n\u003Cblockquote\u003E\n\u003Cp\u003EAs a Kubernetes-native Chaos Engineering practitioner, how do you believe experiments should be?\u003Cbr\u003E.\u003Cbr\u003E.\u003Cbr\u003EThoughts from \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fkubernauts?ref_src=twsrc%5Etfw\"\u003E@kubernauts\u003C\u002Fa\u003E ?\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FChaosEngineering?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#ChaosEngineering\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FKubernetes?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Kubernetes\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FOSS?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#OSS\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDevOps?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#DevOps\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FSRE?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#SRE\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FCloudNative?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#CloudNative\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDocker?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Docker\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk8s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k8s\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk8sjp?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k8sjp\u003C\u002Fa\u003E\u003C\u002Fp\u003E— LitmusChaos (Chaos Engineering for Kubernetes) (\u003Ca class=\"mentioned-user\" href=\"https:\u002F\u002Fdev.to\u002Flitmuschaos\"\u003E@litmuschaos\u003C\u002Fa\u003E\n) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FLitmusChaos\u002Fstatus\u002F1285587689148829696?ref_src=twsrc%5Etfw\"\u003EJuly 21, 2020\u003C\u002Fa\u003E\n\u003C\u002Fblockquote\u003E \n\n\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003EThe control offered by the chaos-frameworks is extremely important to the SRE. That encompasses the flexibility to choose security configuration, isolate chaos resources, abort\u002Fstop chaos when needed, and even simplicity in defining new experiments (Hear Andreas Krivas from Container Solutions talking about it based on prod experience in this \u003Ca href=\"https:\u002F\u002Fyoutu.be\u002FC57WqBU-Vts?t=647\"\u003Euser-interview\u003C\u002Fa\u003E)\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EAnother trend, something that is nascent is the application of chaos engineering to the edge. Kubernetes for the edge is something that has caught on in recent times, and chaos engineering is a natural extension! This calls for newer experiments, multi-arch support &amp; resource-optimized deployments. This \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fblob\u002Fmaster\u002Fadopters\u002Forganizations\u002Fraspbernetes.md\"\u003Eadoption story\u003C\u002Fa\u003E from Michael Fornaro (founder\u002Fmaintainer of \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fraspbernetes\"\u003ERaspbernetes\u003C\u002Fa\u003E) is an early indicator! Something that has driven the litmus e2e team to prioritize platforms such as K3s as part of its e2e suite. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EWhile Kubernetes native chaos is something that is appreciated and has caught on, the need to inject failures on infrastructure components falling “outside” the ambit of the Kube API continues to be in demand. And probably is ever-more important. This includes support for node-level and disk-level failures, with growing requests for interoperability with different cloud providers. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\n\u003Cblockquote\u003E\n\u003Cp\u003EWhich of these \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FKubernetes?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Kubernetes\u003C\u002Fa\u003E Cluster would you prefer to use in your local development or CI?\u003Cbr\u003E---\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FLitmusChaos?ref_src=twsrc%5Etfw\"\u003E@LitmusChaos\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fkind?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#kind\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fmicrok8s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#microk8s\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fkubernetesio?ref_src=twsrc%5Etfw\"\u003E@kubernetesio\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDocker?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Docker\u003C\u002Fa\u003E\u003Cbr\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk3s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k3s\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FRancher_Labs?ref_src=twsrc%5Etfw\"\u003E@Rancher_Labs\u003C\u002Fa\u003E\u003C\u002Fp\u003E— Udit Gaurav (@udit_gaurav15) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fudit_gaurav15\u002Fstatus\u002F1306938070466310144?ref_src=twsrc%5Etfw\"\u003ESeptember 18, 2020\u003C\u002Fa\u003E\n\u003C\u002Fblockquote\u003E \n\u003Ch1\u003E\n  \u003Ca name=\"integrations-with-other-projects-in-the-cncf-landscape\" href=\"#integrations-with-other-projects-in-the-cncf-landscape\"\u003E\n  \u003C\u002Fa\u003E\n  Integrations with Other Projects in the CNCF Landscape\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EWith chaos engineering being accepted as an important cog in the application delivery process (Litmus is part of the App-Delivery SIG in CNCF), it was important that we interface with standard cloud-native mechanisms enabling it. Right from dev-test through CI pipelines, automated validation upon continuous deployment into pre-prod environments (CD) and eventually in production. This resulted in the following integrations, which are expected to undergo a formal release process just as the core chaos framework. \u003C\u002Fp\u003E\n\n\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003EOkteto (DevTest): Okteto simplifies the application development experience and reduces the cycles spent in getting a test image validated on the Kubernetes cluster. It reduces this “inner loop of development” by allowing “in-cluster development”, i.e., helps spin-up a complete dev environment on the cluster itself with access to all the code in your workspace. It has a SaaS offering (Okteto Cloud) that provides Kubernetes namespaces with the option to pull in LitmusChaos control plane so that developers can right away gauge the impact of failures and fix them before hitting CI. You can leverage Okteto’s \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fokteto\u002Fchaos-engineering-with-litmus-and-okteto-cloud-2p3l\"\u003Elitmus-enabled preconfigured development\u003C\u002Fa\u003E environments to deploy Litmus along with your application with a single click. On the other hand, Okteto is also being actively used for the \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fksatchit\u002Flitmus-sdk-devtest-your-chaos-experiments-with-okteto-4dkj\"\u003Edevelopment of the chaos experiments\u003C\u002Fa\u003E themselves!\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EGitlab (CI): Running chaos experiments, albeit with “stricter” validations, not unlike failure tests is something that is catching up. Litmus provides \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fuditgaurav\u002Flitmuschaos-gitlab-remote-templates-6l2\"\u003Echaos templates\u003C\u002Fa\u003E for Gitlab, an abstraction over the chaos CRDs to extend your regular CI pipelines with new stages to inject various types of faults and validate for a specific application or infra behavior. The usage models here are varied, with some teams opting for quick litmus-based e2e on low-footprint ephemeral clusters such as KIND that can be brought up within the build environment itself, thereby not needing to push test images to a registry v\u002Fs more elaborate “retrospective” pipelines running against cloud\u002Fhosted K8s clusters after the images are pushed to a test repository. \u003Ca href=\"https:\u002F\u002Fopenebs.ci\"\u003Eopenebs.ci\u003C\u002Fa\u003E is a great example of the latter.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EArgo (Workflows): Argo is an incubating project in CNCF with a suite of sub-projects under its banner (workflows, CD, events, rollouts). Amongst those, the community (notably Intuit) foresaw great value in creating what we are now calling \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fksatchit\u002Fchaos-workflows-with-argo-and-litmuschaos-2po5\"\u003E“chaos workflows”\u003C\u002Fa\u003E, which are essentially argo workflows running one or more experiments, often in combination with tools to generate “real world load”. This enables the creation of complex chaos scenarios (for ex: you could simulate parallel multiple-component failures, chained chaos - i.e., cases where a sequence of certain failures can cause unique issues, etc.,) and imparts greater power to developers and SREs. The chaos workflows are different from vanilla argo workflows in the sense that the status of a workflow is determined by the success of the chaos experiments. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EKeptn (CD): Keptn is an open-source cloud-native application life-cycle orchestration tool. An entrant into the CNCF sandbox alongside Litmus, a major use case of Keptn within Kubernetes involves defining a pipeline with one or more stages for deployment, testing, and remediation strategies. It provides a mechanism to implement a Quality Gate for the promotion of applications based on SLO validation. Litmus \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fksatchit\u002Fpart-1-evaluating-resiliency-with-keptn-and-litmuschaos-30jo\"\u003Eintegrates\u003C\u002Fa\u003E with Keptn (via a dedicated \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fkeptn-sandbox\u002Flitmus-service\"\u003Elitmus-service\u003C\u002Fa\u003E in the Keptn control plane) to inject chaos into any stage of the Keptn pipeline, especially with background load to simulate real-world behavior, thereby lending greater strength to the SLO eval process. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003ESpinnaker (CD): In recent times, Spinnaker has introduced the \u003Ca href=\"https:\u002F\u002Fwww.armory.io\u002Fblog\u002Fspinnaker-extensibility-new-heights-plugins\u002F\"\u003Eplugin\u003C\u002Fa\u003E model to extend the capabilities of this popular continuous delivery platform, while keeping a lean core. With an increasing number of organizations using spinnaker for the CD needs, we introduced the \u003Ca href=\"https:\u002F\u002Fwww.armory.io\u002Fblog\u002Flitmuschaos-in-your-spinnaker-pipeline\u002F\"\u003ELitmus plugin\u003C\u002Fa\u003E that enables the creation of a custom stage for chaos, with the ability to inject different failures on the deployed application and creating opinionated exit checks. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"notable-features-we-added\" href=\"#notable-features-we-added\"\u003E\n  \u003C\u002Fa\u003E\n  Notable Features We Added\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EAs mentioned earlier, a lot of the new enhancements &amp; features that we added during this period were a result of community requests and roadmap items crafted based on the trends observed. Here is a high-level list of some notable improvements: \u003C\u002Fp\u003E\n\n\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003EThe existing experiments were migrated to golang, and (12) newer ones added (including node-level chaos experiments). We also simplified the experiment development process via the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus-go\u002Ftree\u002Fmaster\u002Fcontribute\u002Fdeveloper-guide\"\u003Elitmus-sdk\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EThe \u003Ca href=\"https:\u002F\u002Fdocs.litmuschaos.io\u002Fdocs\u002Fchaosengine\u002F\"\u003EChaosEngine\u003C\u002Fa\u003E CRD schema has been upgraded significantly to support various properties for the chaos pods, leading to granular control of the experiment (including pod\u002Fcontainer-security context, image overrides &amp; secrets, resource requests-limits, pod scheduling via selectors &amp; tolerations, ability to inject custom labels &amp; annotations, etc.,)\u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003ESupport for chaos on runtimes other than docker (containerd, crio) \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fdev.to\u002Fispeakc0de\u002Fdeclarative-approach-to-chaos-hypothesis-using-litmus-probes-5157\"\u003ELitmus Probes\u003C\u002Fa\u003E were introduced to enable declarative steady-state definition and construct hypotheses \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EThe \u003Ca href=\"https:\u002F\u002Fdocs.litmuschaos.io\u002Fdocs\u002Fscheduling\u002F\"\u003EChaos-Scheduler\u003C\u002Fa\u003E was made available to help with the execution of continuous background chaos. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003ESupport for newer \u003Ca href=\"https:\u002F\u002Fdev.to\u002Fksatchit\u002Fdeployment-modes-in-litmuschaos-1bpa\"\u003Emodes of operation\u003C\u002Fa\u003E (admin, namespaced) to enable centralization of chaos &amp; operations in multi-tenant environments respectively \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EThe CI &amp; e2e practices in the litmus project have improved (though there is much more to be done) over time, with support for multi-arch builds, test runs for pull request validation as well as increased \u003Ca href=\"https:\u002F\u002Flitmuschaos.github.io\u002Flitmus-e2e\u002Findex\"\u003Eintegration test coverage\u003C\u002Fa\u003E for the litmus control plane and chaos experiments. \u003C\u002Fp\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Cp\u003ENot least, we also introduced the chaos-bird as our new mascot !! \u003C\u002Fp\u003E\n\n\n\u003Cblockquote\u003E\n\u003Cp\u003EHola k8s folks, Unveiling the brand new mascot for Litmus, the Chaos Bird in grand fashion \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FKubeCon_?ref_src=twsrc%5Etfw\"\u003E@KubeCon_\u003C\u002Fa\u003E. The all new website (\u003Ca href=\"https:\u002F\u002Ft.co\u002FhDS6S1xp9d\"\u003E\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Ft.co\u002FhDS6S1xp9d\"\u003Ehttps:\u002F\u002Ft.co\u002FhDS6S1xp9d\u003C\u002Fa\u003E) and ChaosHub (\u003Ca href=\"https:\u002F\u002Ft.co\u002FBA4angVyNu\"\u003E\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Ft.co\u002FBA4angVyNu\"\u003Ehttps:\u002F\u002Ft.co\u002FBA4angVyNu\u003C\u002Fa\u003E) are all about the Chaos Bird unfurling its magic in the cloud-native world!\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fchaosengineering?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#chaosengineering\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ft.co\u002FB3tVsqzwGR\"\u003Epic.twitter.com\u002FB3tVsqzwGR\u003C\u002Fa\u003E\u003C\u002Fp\u003E— Ishan Gupta (\u003Ca class=\"mentioned-user\" href=\"https:\u002F\u002Fdev.to\u002Fcode_igx\"\u003E@code_igx\u003C\u002Fa\u003E\n) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fcode_igx\u002Fstatus\u002F1295323815522656257?ref_src=twsrc%5Etfw\"\u003EAugust 17, 2020\u003C\u002Fa\u003E\n\u003C\u002Fblockquote\u003E \n\u003Ch1\u003E\n  \u003Ca name=\"introducing-the-litmus-portal\" href=\"#introducing-the-litmus-portal\"\u003E\n  \u003C\u002Fa\u003E\n  Introducing the Litmus Portal\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EOnce we established the fact to ourselves and the community that chaos workflows are the way forward in terms of “scaling” chaos (i.e., increase injections with different “ordering” schemes to form complex scenarios), we decided to simplify this experience for users. After all, constructing YAMLs spanning hundreds of lines by hand is something that is universally detested! Over time, we also learned that “scaling” chaos has other connotations too - such as being able to inject chaos across several “target” clusters from a single “control plane”, extending the chaos execution &amp; analysis to a team or groups of engineers, etc., Not just that, chaos engineering is a lot about visualization, introspection, and analysis of results as against just fault-injection. \u003C\u002Fp\u003E\n\n\u003Cp\u003EAll this resulted in the creation of the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Ftree\u002Fmaster\u002Flitmus-portal\"\u003ELitmus Portal\u003C\u002Fa\u003E which offers a centralized management console to construct, schedule, and visualize chaos workflows on multiple clusters, connected to it via an agent\u002Fsubscriber. It allows selecting chaos experiments from a trusted\u002Fspecific git source and offers insights into application\u002Finfrastructure resiliency via a “resilience grading” mechanism that allows setting weights to experiment results. It also supports the generation of chaos reports and a git-based auth mechanism to aid chaos execution by teams. The portal is alpha-2 at the time of writing this article and is expected to evolve to include many other features over time.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--rEK4fzxs--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fr3i5l31z3vpbjot4jbtt.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--rEK4fzxs--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fr3i5l31z3vpbjot4jbtt.png\" alt=\"Alt Text\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs---WKRIcQI--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fi.imgur.com\u002FWdwzfs0.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs---WKRIcQI--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fi.imgur.com\u002FWdwzfs0.png\" alt=\"Alt Text\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"state-of-governance\" href=\"#state-of-governance\"\u003E\n  \u003C\u002Fa\u003E\n  State of Governance\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EOne of the areas we made significant strides in this period was project governance. While Litmus had become collaborative and “truly” opensource when the maintainer\u002Fproject leadership group grew to include \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsumitnagal\"\u003ESumit Nagal\u003C\u002Fa\u003E (Intuit), \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fk8s-dev\"\u003EJayesh Kumar\u003C\u002Fa\u003E(AWS) &amp; \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FJasstkn\"\u003EMaria Kotlyarevskaya\u003C\u002Fa\u003E (Wrike) apart from the team at MayaData, we were looking for ways to decentralize the planning &amp; maintenance activities of the various sub-projects within Litmus. We also realized, that over time, Litmus was being perceived and used in a variety of use-cases - while some folks in the community were focused on integrations with other tools\u002Fframeworks, few were interested in improving observability hooks and yet others wanted to focus on improving the chaos orchestration logic to improve the chaos experience. We also gauged interest amongst some members in learning about and contributing to the project by improving the documentation. \u003C\u002Fp\u003E\n\n\u003Cp\u003EThis led us to create \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fwiki\u002FSpecial-Interest-Groups\"\u003ESpecial Interest Groups (SIGs)\u003C\u002Fa\u003E within the Litmus project (something that CNCF &amp; Kubernetes has implemented to great effect) to allow the community members to align themselves with like-minded individuals and improve the project by defining the direction in which the area\u002Fsub-projects should go. Each SIG is composed of a group of community members led by at least two SIG-Chairs who also have the commit bits to a set of github repositories that come under the purview of the area\u002Fsub-projects. Each SIG has a set of well-defined Goals, Non-Goals &amp; is also responsible for proposing deliverables for the monthly releases. The SIG teams typically meet once in two weeks to discuss progress, roadmap items and to engage in cool demos\u002Fpresentations. While some SIGs are formally operational (Documentation, Observability, Orchestration, Integrations), a few others are yet to take off (Deployment, Testing, CI). We hope these will soon be functional! \u003C\u002Fp\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"looking-ahead\" href=\"#looking-ahead\"\u003E\n  \u003C\u002Fa\u003E\n  Looking Ahead\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EWe recognize the fact that there is increasing adoption of chaos engineering as a practice across organizations, and sustained improvement is key in ensuring the cloud-native world is able to leverage litmus for their resilience needs successfully. The project \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fblob\u002Fmaster\u002FROADMAP.md\"\u003Eroadmap\u003C\u002Fa\u003E is continuously being upgraded to hold newer requirements and stability improvements. To this effect, we have been planning a steering committee composed of people from the resilience, observability &amp; application delivery world that can help prioritize features, enrich project\u002Fproduct management practices, and generally help bring real-world experience to improve Litmus in becoming a well-rounded solution! If you are interested, do reach out to us on the \u003Ca href=\"https:\u002F\u002Fslack.litmuschaos.io\"\u003Elitmus slack\u003C\u002Fa\u003E channel. \u003C\u002Fp\u003E\n\n\u003Cp\u003ESo, to summarize, we are looking at the community getting involved in a bigger way in driving the project - via feedback and contributions. Needless to say, we are also focused on moving further along the path as a CNCF project which would accelerate achieving these goals. The next Kubecon is 6 months away, but we \u003Cem\u003Ewill\u003C\u002Fem\u003E keep you all posted on what's new in Litmus.\u003C\u002Fp\u003E\n\n\u003Cp\u003EBefore we end this post, it is worth looking at what Liz Rice, the TOC (Technical Oversight Committee) chair at CNCF had to say about chaos engineering! \u003C\u002Fp\u003E\n\n\n\u003Cblockquote\u003E\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FCNCF?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#CNCF\u003C\u002Fa\u003E TOC chair \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Flizrice?ref_src=twsrc%5Etfw\"\u003E@lizrice\u003C\u002Fa\u003E is sharing the 5 technologies to watch in 2021 according to the TOC:\u003Cbr\u003E1. Chaos engineering\u003Cbr\u003E2. \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fkubernetesio?ref_src=twsrc%5Etfw\"\u003E@kubernetesio\u003C\u002Fa\u003E for the edge\u003Cbr\u003E3. Service mesh\u003Cbr\u003E4. Web assembly and eBPF\u003Cbr\u003E5. Developer + operator experience \u003Ca href=\"https:\u002F\u002Ft.co\u002FaSRDTB0piN\"\u003Epic.twitter.com\u002FaSRDTB0piN\u003C\u002Fa\u003E\u003C\u002Fp\u003E— CNCF (@CloudNativeFdn) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FCloudNativeFdn\u002Fstatus\u002F1329863326428499971?ref_src=twsrc%5Etfw\"\u003ENovember 20, 2020\u003C\u002Fa\u003E\n\u003C\u002Fblockquote\u003E \n\n\u003Chr\u003E\n\n\u003Cp\u003EAre you an SRE, developer, or a Kubernetes enthusiast? Does Chaos Engineering excite you? Join our community on \u003Ca href=\"\u002F\u002Fslack.litmuschaos.io\"\u003ESlack\u003C\u002Fa\u003E For detailed discussions &amp; regular updates On Chaos Engineering For Kubernetes.\u003C\u002Fp\u003E\n\n\u003Cp\u003ECheck out the LitmusChaos \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\"\u003EGitHub repo\u003C\u002Fa\u003E and do share your feedback. Submit a pull request if you identify any necessary changes.\u003C\u002Fp\u003E\n\n";c.body_markdown="This blog’s title suggests that it may have arrived a month early. After all, reflections are mostly written at year-end with predictions and goals for the new year. But to those of us accustomed to compartmentalizing our lives around the festival called “KubeCon”, this doesn’t seem out of place. Also, the choice of six months is thanks to the fact that we have been a CNCF sandbox project for (nearly) this duration. While some of us have been lucky to witness first-hand the ongoings in the project from a vantage point as maintainers, we felt the need for us to share them with the larger litmus community. So, this article is going to be a mix of updates as well as opinions\u002Ftrends we are observing in the cloud-native chaos engineering space, with generous references to specific docs\u002Fblog posts\u002Fvideos highlighting them. \n\n# The Project Has Grown...\n\n...And how! Thanks in no small part to being accepted as a CNCF sandbox project. While we were convinced of the need for a cloud-native chaos engineering solution (which is what motivated us to start Litmus in the first place), the reach and vibrant nature of the CNCF developer & end-user community amplified the interest in the project leading to increased contributions from developers across different organizations (including RedHat, Intuit, ContainerSolutions, Microsoft, etc.,) as well as new adopters (Okteto, WeScale, NetApp). It also paved the way for greater collaboration & integrations with other projects on the CNCF landscape that are focused on solving different kinds of challenges around application delivery (for ex: Okteto, Argo, Keptn, Spinnaker), about which we shall delve a bit more in subsequent sections. More importantly, it has helped generate more dialogue with diverse folks who are at various stages of the cloud-native journey - right from those that have crossed the chasm & have turned into mature\u002Fwell-oiled adopters of the paradigm to those that are in the process of re-architecting their applications into microservices and migrating them onto their first Kubernetes staging clusters. The Litmus slack channel has never been more active (it saw a 70%+ increase in members), and we are also grateful for all those on-demand zoom calls and pages-worth slack conversations - it would be an understatement to say that more than 90% of the features\u002Fenhancements and fixes that went in during this period were direct end-results of this dialogue. Talk about being community-driven!! \n\n![Alt Text](https:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002F3do1hmvjxyaqcp13ad4c.png)\n\n# Evolution Of (Cloud-Native) Chaos Engineering \n\nWhile the principles around which we helped define the [_sub-category_](https:\u002F\u002Fwww.cncf.io\u002Fblog\u002F2019\u002F11\u002F06\u002Fcloud-native-chaos-engineering-enhancing-kubernetes-application-resiliency\u002F) of cloud-native chaos engineering (a little over a year ago) continue to hold true (validated by the emergence of other projects sharing similar values), we have noticed some developments in this space since. \n\n*These are also things that influenced us as we developed and iterated on the litmus framework, resulting in several new features & integrations.* \n\nProminent amongst those has been the call for “left-shift” of chaos - which is now being increasingly viewed as part of the delivery pipelines as against something run only in production environments, by SRE or Ops persona. This is definitely an interesting departure from traditionally held views about CE. While the “exploratory” model of chaos engineering with controlled failures on prod & well-defined hypotheses in a gameday setting is still the Nirvana of a mature devops practice, we found an urgent need amongst the community for doing it much earlier in an automated way (read CI\u002FCD pipelines) in the wake of overhauls to application architecture (making it microservices oriented) & their impending\u002Fongoing migration to Kubernetes.\n\n*Note that this is not to say chaos engineering doesn’t apply to monolithic apps or non-Kubernetes environments, just that the recent shift to a cloud-native mindset has brought about new practices.*\n\n\u003Cblockquote class=\"twitter-tweet\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EWhere do you use or intend to use Chaos Engineering in your Kubernetes architecture?\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fchaosengineering?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#chaosengineering\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FKubernetes?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Kubernetes\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fcncf?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#cncf\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDevOps?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#DevOps\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk8s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k8s\u003C\u002Fa\u003E\u003C\u002Fp\u003E&mdash; LitmusChaos (Chaos Engineering for Kubernetes) (@LitmusChaos) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FLitmusChaos\u002Fstatus\u002F1274978096106532864?ref_src=twsrc%5Etfw\"\u003EJune 22, 2020\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E \u003Cscript async src=\"https:\u002F\u002Fplatform.twitter.com\u002Fwidgets.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\nA [panel discussion](https:\u002F\u002Fyoutu.be\u002Fs-8HwXMdeMk?t=786) we curated recently to dig more perspective around this topic and other trends in this space was especially enlightening. So have been the conversations with different users (the persona ranging from application developers to people classifying themselves as “devops engineers” focused on delivery tooling, QA architects, SREs, VPs & CTOs). Here is a summarization: \n\n- Reliability verification is needed right from the earliest stages of application\u002Fplatform development. Something that is being termed as the “Chaos First” principle \n\n\u003Cblockquote class=\"twitter-tweet\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EChaos first is a really important principle. It’s too hard to add resilience to a finished project. Needs to be a gate for deployment in the first place. \u003Ca href=\"https:\u002F\u002Ft.co\u002Fxi98y8JZpJ\"\u003Ehttps:\u002F\u002Ft.co\u002Fxi98y8JZpJ\u003C\u002Fa\u003E\u003C\u002Fp\u003E&mdash; adrian cockcroft (@adrianco) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fadrianco\u002Fstatus\u002F1201703004014907392?ref_src=twsrc%5Etfw\"\u003EDecember 3, 2019\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E \u003Cscript async src=\"https:\u002F\u002Fplatform.twitter.com\u002Fwidgets.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\n- Chaos to “test” or unearth issues in the observability frameworks is being increasingly seen as a way to lower the entry-barrier for chaos engineering in many organizations that are generally apprehensive about the idea. Litmus has seen adoption in multiple organizations with this use-case. In some cases, this is the main motivation too (for ex: testing log-based anomaly detection in [Zebrium](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fblob\u002Fmaster\u002Fadopters\u002Forganizations\u002Fzebrium.md#how-do-we-use-litmus)!)\n\n- There is a definite need identified to bring in methods to define steady-state of systems & hypothesize about the impact of chaos in a declarative way, without which the advantage of a Kubernetes-native\u002FYAML based approach is nullified. This is a prerequisite for automated SLO validation.\n\n- Organic integrations emerge between cloud-native chaos toolsets & observability frameworks, with native support for chaos events, metrics (and other elements). Observability was & continues to be a prerequisite for chaos engineering. \n\n- The “run-everywhere” drive in chaos engineering has brought it to the developer’s doorstep and has become a much-used strategy in dev tests. This, combined with the self-service model where developers are provided “namespaces” with “quotas”, has resulted in the need for chaos frameworks to be “multi-tenant” aware, with implications right from RBACs needed for experimentation to the resource consumption at scale.\n\n\u003Cblockquote class=\"twitter-tweet\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EAs a Kubernetes-native Chaos Engineering practitioner, how do you believe experiments should be?\u003Cbr\u003E.\u003Cbr\u003E.\u003Cbr\u003EThoughts from \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fkubernauts?ref_src=twsrc%5Etfw\"\u003E@kubernauts\u003C\u002Fa\u003E ?\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FChaosEngineering?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#ChaosEngineering\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FKubernetes?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Kubernetes\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FOSS?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#OSS\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDevOps?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#DevOps\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FSRE?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#SRE\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FCloudNative?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#CloudNative\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDocker?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Docker\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk8s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k8s\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk8sjp?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k8sjp\u003C\u002Fa\u003E\u003C\u002Fp\u003E&mdash; LitmusChaos (Chaos Engineering for Kubernetes) (@LitmusChaos) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FLitmusChaos\u002Fstatus\u002F1285587689148829696?ref_src=twsrc%5Etfw\"\u003EJuly 21, 2020\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E \u003Cscript async src=\"https:\u002F\u002Fplatform.twitter.com\u002Fwidgets.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\n- The control offered by the chaos-frameworks is extremely important to the SRE. That encompasses the flexibility to choose security configuration, isolate chaos resources, abort\u002Fstop chaos when needed, and even simplicity in defining new experiments (Hear Andreas Krivas from Container Solutions talking about it based on prod experience in this [user-interview](https:\u002F\u002Fyoutu.be\u002FC57WqBU-Vts?t=647))\n\n- Another trend, something that is nascent is the application of chaos engineering to the edge. Kubernetes for the edge is something that has caught on in recent times, and chaos engineering is a natural extension! This calls for newer experiments, multi-arch support & resource-optimized deployments. This [adoption story](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fblob\u002Fmaster\u002Fadopters\u002Forganizations\u002Fraspbernetes.md) from Michael Fornaro (founder\u002Fmaintainer of [Raspbernetes](https:\u002F\u002Fgithub.com\u002Fraspbernetes)) is an early indicator! Something that has driven the litmus e2e team to prioritize platforms such as K3s as part of its e2e suite. \n\n- While Kubernetes native chaos is something that is appreciated and has caught on, the need to inject failures on infrastructure components falling “outside” the ambit of the Kube API continues to be in demand. And probably is ever-more important. This includes support for node-level and disk-level failures, with growing requests for interoperability with different cloud providers. \n\n\u003Cblockquote class=\"twitter-tweet\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EWhich of these \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FKubernetes?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Kubernetes\u003C\u002Fa\u003E Cluster would you prefer to use in your local development or CI?\u003Cbr\u003E---\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FLitmusChaos?ref_src=twsrc%5Etfw\"\u003E@LitmusChaos\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fkind?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#kind\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fmicrok8s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#microk8s\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fkubernetesio?ref_src=twsrc%5Etfw\"\u003E@kubernetesio\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FDocker?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#Docker\u003C\u002Fa\u003E\u003Cbr\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fk3s?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#k3s\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FRancher_Labs?ref_src=twsrc%5Etfw\"\u003E@Rancher_Labs\u003C\u002Fa\u003E\u003C\u002Fp\u003E&mdash; Udit Gaurav (@udit_gaurav15) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fudit_gaurav15\u002Fstatus\u002F1306938070466310144?ref_src=twsrc%5Etfw\"\u003ESeptember 18, 2020\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E \u003Cscript async src=\"https:\u002F\u002Fplatform.twitter.com\u002Fwidgets.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\n# Integrations with Other Projects in the CNCF Landscape \n\n\nWith chaos engineering being accepted as an important cog in the application delivery process (Litmus is part of the App-Delivery SIG in CNCF), it was important that we interface with standard cloud-native mechanisms enabling it. Right from dev-test through CI pipelines, automated validation upon continuous deployment into pre-prod environments (CD) and eventually in production. This resulted in the following integrations, which are expected to undergo a formal release process just as the core chaos framework. \n\n- Okteto (DevTest): Okteto simplifies the application development experience and reduces the cycles spent in getting a test image validated on the Kubernetes cluster. It reduces this “inner loop of development” by allowing “in-cluster development”, i.e., helps spin-up a complete dev environment on the cluster itself with access to all the code in your workspace. It has a SaaS offering (Okteto Cloud) that provides Kubernetes namespaces with the option to pull in LitmusChaos control plane so that developers can right away gauge the impact of failures and fix them before hitting CI. You can leverage Okteto’s [litmus-enabled preconfigured development](https:\u002F\u002Fdev.to\u002Fokteto\u002Fchaos-engineering-with-litmus-and-okteto-cloud-2p3l) environments to deploy Litmus along with your application with a single click. On the other hand, Okteto is also being actively used for the [development of the chaos experiments](https:\u002F\u002Fdev.to\u002Fksatchit\u002Flitmus-sdk-devtest-your-chaos-experiments-with-okteto-4dkj) themselves!\n\n- Gitlab (CI): Running chaos experiments, albeit with “stricter” validations, not unlike failure tests is something that is catching up. Litmus provides [chaos templates](https:\u002F\u002Fdev.to\u002Fuditgaurav\u002Flitmuschaos-gitlab-remote-templates-6l2) for Gitlab, an abstraction over the chaos CRDs to extend your regular CI pipelines with new stages to inject various types of faults and validate for a specific application or infra behavior. The usage models here are varied, with some teams opting for quick litmus-based e2e on low-footprint ephemeral clusters such as KIND that can be brought up within the build environment itself, thereby not needing to push test images to a registry v\u002Fs more elaborate “retrospective” pipelines running against cloud\u002Fhosted K8s clusters after the images are pushed to a test repository. [openebs.ci](https:\u002F\u002Fopenebs.ci) is a great example of the latter.\n\n- Argo (Workflows): Argo is an incubating project in CNCF with a suite of sub-projects under its banner (workflows, CD, events, rollouts). Amongst those, the community (notably Intuit) foresaw great value in creating what we are now calling [“chaos workflows”](https:\u002F\u002Fdev.to\u002Fksatchit\u002Fchaos-workflows-with-argo-and-litmuschaos-2po5), which are essentially argo workflows running one or more experiments, often in combination with tools to generate “real world load”. This enables the creation of complex chaos scenarios (for ex: you could simulate parallel multiple-component failures, chained chaos - i.e., cases where a sequence of certain failures can cause unique issues, etc.,) and imparts greater power to developers and SREs. The chaos workflows are different from vanilla argo workflows in the sense that the status of a workflow is determined by the success of the chaos experiments. \n\n- Keptn (CD): Keptn is an open-source cloud-native application life-cycle orchestration tool. An entrant into the CNCF sandbox alongside Litmus, a major use case of Keptn within Kubernetes involves defining a pipeline with one or more stages for deployment, testing, and remediation strategies. It provides a mechanism to implement a Quality Gate for the promotion of applications based on SLO validation. Litmus [integrates](https:\u002F\u002Fdev.to\u002Fksatchit\u002Fpart-1-evaluating-resiliency-with-keptn-and-litmuschaos-30jo) with Keptn (via a dedicated [litmus-service](https:\u002F\u002Fgithub.com\u002Fkeptn-sandbox\u002Flitmus-service) in the Keptn control plane) to inject chaos into any stage of the Keptn pipeline, especially with background load to simulate real-world behavior, thereby lending greater strength to the SLO eval process. \n\n- Spinnaker (CD): In recent times, Spinnaker has introduced the [plugin](https:\u002F\u002Fwww.armory.io\u002Fblog\u002Fspinnaker-extensibility-new-heights-plugins\u002F) model to extend the capabilities of this popular continuous delivery platform, while keeping a lean core. With an increasing number of organizations using spinnaker for the CD needs, we introduced the [Litmus plugin](https:\u002F\u002Fwww.armory.io\u002Fblog\u002Flitmuschaos-in-your-spinnaker-pipeline\u002F) that enables the creation of a custom stage for chaos, with the ability to inject different failures on the deployed application and creating opinionated exit checks. \n\n# Notable Features We Added \n\nAs mentioned earlier, a lot of the new enhancements & features that we added during this period were a result of community requests and roadmap items crafted based on the trends observed. Here is a high-level list of some notable improvements: \n\n- The existing experiments were migrated to golang, and (12) newer ones added (including node-level chaos experiments). We also simplified the experiment development process via the [litmus-sdk](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus-go\u002Ftree\u002Fmaster\u002Fcontribute\u002Fdeveloper-guide)\n\n- The [ChaosEngine](https:\u002F\u002Fdocs.litmuschaos.io\u002Fdocs\u002Fchaosengine\u002F) CRD schema has been upgraded significantly to support various properties for the chaos pods, leading to granular control of the experiment (including pod\u002Fcontainer-security context, image overrides & secrets, resource requests-limits, pod scheduling via selectors & tolerations, ability to inject custom labels & annotations, etc.,)\n\n- Support for chaos on runtimes other than docker (containerd, crio) \n\n- [Litmus Probes](https:\u002F\u002Fdev.to\u002Fispeakc0de\u002Fdeclarative-approach-to-chaos-hypothesis-using-litmus-probes-5157) were introduced to enable declarative steady-state definition and construct hypotheses \n\n- The [Chaos-Scheduler](https:\u002F\u002Fdocs.litmuschaos.io\u002Fdocs\u002Fscheduling\u002F) was made available to help with the execution of continuous background chaos. \n\n- Support for newer [modes of operation](https:\u002F\u002Fdev.to\u002Fksatchit\u002Fdeployment-modes-in-litmuschaos-1bpa) (admin, namespaced) to enable centralization of chaos & operations in multi-tenant environments respectively \n\n- The CI & e2e practices in the litmus project have improved (though there is much more to be done) over time, with support for multi-arch builds, test runs for pull request validation as well as increased [integration test coverage](https:\u002F\u002Flitmuschaos.github.io\u002Flitmus-e2e\u002Findex) for the litmus control plane and chaos experiments. \n\nNot least, we also introduced the chaos-bird as our new mascot !! \n\n\u003Cblockquote class=\"twitter-tweet\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EHola k8s folks, Unveiling the brand new mascot for Litmus, the Chaos Bird in grand fashion \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FKubeCon_?ref_src=twsrc%5Etfw\"\u003E@KubeCon_\u003C\u002Fa\u003E. The all new website (\u003Ca href=\"https:\u002F\u002Ft.co\u002FhDS6S1xp9d\"\u003Ehttps:\u002F\u002Ft.co\u002FhDS6S1xp9d\u003C\u002Fa\u003E) and ChaosHub (\u003Ca href=\"https:\u002F\u002Ft.co\u002FBA4angVyNu\"\u003Ehttps:\u002F\u002Ft.co\u002FBA4angVyNu\u003C\u002Fa\u003E) are all about the Chaos Bird unfurling its magic in the cloud-native world!\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002Fchaosengineering?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#chaosengineering\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Ft.co\u002FB3tVsqzwGR\"\u003Epic.twitter.com\u002FB3tVsqzwGR\u003C\u002Fa\u003E\u003C\u002Fp\u003E&mdash; Ishan Gupta (@code_igx) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fcode_igx\u002Fstatus\u002F1295323815522656257?ref_src=twsrc%5Etfw\"\u003EAugust 17, 2020\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E \u003Cscript async src=\"https:\u002F\u002Fplatform.twitter.com\u002Fwidgets.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\n# Introducing the Litmus Portal \n\nOnce we established the fact to ourselves and the community that chaos workflows are the way forward in terms of “scaling” chaos (i.e., increase injections with different “ordering” schemes to form complex scenarios), we decided to simplify this experience for users. After all, constructing YAMLs spanning hundreds of lines by hand is something that is universally detested! Over time, we also learned that “scaling” chaos has other connotations too - such as being able to inject chaos across several “target” clusters from a single “control plane”, extending the chaos execution & analysis to a team or groups of engineers, etc., Not just that, chaos engineering is a lot about visualization, introspection, and analysis of results as against just fault-injection. \n\nAll this resulted in the creation of the [Litmus Portal](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Ftree\u002Fmaster\u002Flitmus-portal) which offers a centralized management console to construct, schedule, and visualize chaos workflows on multiple clusters, connected to it via an agent\u002Fsubscriber. It allows selecting chaos experiments from a trusted\u002Fspecific git source and offers insights into application\u002Finfrastructure resiliency via a “resilience grading” mechanism that allows setting weights to experiment results. It also supports the generation of chaos reports and a git-based auth mechanism to aid chaos execution by teams. The portal is alpha-2 at the time of writing this article and is expected to evolve to include many other features over time.\n\n![Alt Text](https:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fi\u002Fr3i5l31z3vpbjot4jbtt.png)\n\n![Alt Text](https:\u002F\u002Fi.imgur.com\u002FWdwzfs0.png)\n\n# State of Governance\n\nOne of the areas we made significant strides in this period was project governance. While Litmus had become collaborative and “truly” opensource when the maintainer\u002Fproject leadership group grew to include [Sumit Nagal](https:\u002F\u002Fgithub.com\u002Fsumitnagal) (Intuit), [Jayesh Kumar](https:\u002F\u002Fgithub.com\u002Fk8s-dev)(AWS) & [Maria Kotlyarevskaya](https:\u002F\u002Fgithub.com\u002FJasstkn) (Wrike) apart from the team at MayaData, we were looking for ways to decentralize the planning & maintenance activities of the various sub-projects within Litmus. We also realized, that over time, Litmus was being perceived and used in a variety of use-cases - while some folks in the community were focused on integrations with other tools\u002Fframeworks, few were interested in improving observability hooks and yet others wanted to focus on improving the chaos orchestration logic to improve the chaos experience. We also gauged interest amongst some members in learning about and contributing to the project by improving the documentation. \n\nThis led us to create [Special Interest Groups (SIGs)](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fwiki\u002FSpecial-Interest-Groups) within the Litmus project (something that CNCF & Kubernetes has implemented to great effect) to allow the community members to align themselves with like-minded individuals and improve the project by defining the direction in which the area\u002Fsub-projects should go. Each SIG is composed of a group of community members led by at least two SIG-Chairs who also have the commit bits to a set of github repositories that come under the purview of the area\u002Fsub-projects. Each SIG has a set of well-defined Goals, Non-Goals & is also responsible for proposing deliverables for the monthly releases. The SIG teams typically meet once in two weeks to discuss progress, roadmap items and to engage in cool demos\u002Fpresentations. While some SIGs are formally operational (Documentation, Observability, Orchestration, Integrations), a few others are yet to take off (Deployment, Testing, CI). We hope these will soon be functional! \n\n# Looking Ahead\n\nWe recognize the fact that there is increasing adoption of chaos engineering as a practice across organizations, and sustained improvement is key in ensuring the cloud-native world is able to leverage litmus for their resilience needs successfully. The project [roadmap](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus\u002Fblob\u002Fmaster\u002FROADMAP.md) is continuously being upgraded to hold newer requirements and stability improvements. To this effect, we have been planning a steering committee composed of people from the resilience, observability & application delivery world that can help prioritize features, enrich project\u002Fproduct management practices, and generally help bring real-world experience to improve Litmus in becoming a well-rounded solution! If you are interested, do reach out to us on the [litmus slack](https:\u002F\u002Fslack.litmuschaos.io) channel. \n\nSo, to summarize, we are looking at the community getting involved in a bigger way in driving the project - via feedback and contributions. Needless to say, we are also focused on moving further along the path as a CNCF project which would accelerate achieving these goals. The next Kubecon is 6 months away, but we *will* keep you all posted on what's new in Litmus.\n\nBefore we end this post, it is worth looking at what Liz Rice, the TOC (Technical Oversight Committee) chair at CNCF had to say about chaos engineering! \n\n\u003Cblockquote class=\"twitter-tweet\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003E\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fhashtag\u002FCNCF?src=hash&amp;ref_src=twsrc%5Etfw\"\u003E#CNCF\u003C\u002Fa\u003E TOC chair \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Flizrice?ref_src=twsrc%5Etfw\"\u003E@lizrice\u003C\u002Fa\u003E is sharing the 5 technologies to watch in 2021 according to the TOC:\u003Cbr\u003E1. Chaos engineering\u003Cbr\u003E2. \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002Fkubernetesio?ref_src=twsrc%5Etfw\"\u003E@kubernetesio\u003C\u002Fa\u003E for the edge\u003Cbr\u003E3. Service mesh\u003Cbr\u003E4. Web assembly and eBPF\u003Cbr\u003E5. Developer + operator experience \u003Ca href=\"https:\u002F\u002Ft.co\u002FaSRDTB0piN\"\u003Epic.twitter.com\u002FaSRDTB0piN\u003C\u002Fa\u003E\u003C\u002Fp\u003E&mdash; CNCF (@CloudNativeFdn) \u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FCloudNativeFdn\u002Fstatus\u002F1329863326428499971?ref_src=twsrc%5Etfw\"\u003ENovember 20, 2020\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E \u003Cscript async src=\"https:\u002F\u002Fplatform.twitter.com\u002Fwidgets.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\n---\n\nAre you an SRE, developer, or a Kubernetes enthusiast? Does Chaos Engineering excite you? Join our community on [Slack](slack.litmuschaos.io) For detailed discussions & regular updates On Chaos Engineering For Kubernetes.\n\nCheck out the LitmusChaos [GitHub repo](https:\u002F\u002Fgithub.com\u002Flitmuschaos\u002Flitmus) and do share your feedback. Submit a pull request if you identify any necessary changes.";c.user={name:"Karthik Satchitanand",username:f,twitter_username:a,github_username:f,website_url:a,profile_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--bIGbm8k9--\u002Fc_fill,f_auto,fl_progressive,h_640,q_auto,w_640\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F405355\u002Fd8cd976f-14a5-4a9a-94e2-4d1ec0d3f8a0.jpeg",profile_image_90:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--BKFAbORY--\u002Fc_fill,f_auto,fl_progressive,h_90,q_auto,w_90\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F405355\u002Fd8cd976f-14a5-4a9a-94e2-4d1ec0d3f8a0.jpeg"};return {data:[{}],fetch:{"data-v-25febe66:0":{article:c}},mutations:[["SET_CURRENT_ARTICLE",c]]}}(null,"2020-11-23T18:21:46Z",{},"https:\u002F\u002Fdev.to\u002Fksatchit\u002Flitmuschaos-a-reflection-on-the-past-six-months-d6a",21,"ksatchit")));