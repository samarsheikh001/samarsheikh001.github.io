window.__NUXT__=(function(a,b,c,d){return {staticAssetsBase:"\u002F_nuxt\u002Fstatic\u002F1634322943",layout:"default",error:a,state:{currentArticle:{type_of:"article",id:822434,title:"7 Microservices Best Practices for Developers",description:"Unless you’ve been developing software in a cave, you’ve probably heard people sing the praises of...",readable_publish_date:"Sep 13",slug:"7-microservices-best-practices-for-developers-ec5",path:"\u002Fmbogan\u002F7-microservices-best-practices-for-developers-ec5",url:b,comments_count:10,public_reactions_count:c,collection_id:a,published_timestamp:d,positive_reactions_count:c,cover_image:a,social_image:"https:\u002F\u002Fdev.to\u002Fsocial_previews\u002Farticle\u002F822434.png",canonical_url:b,created_at:"2021-09-13T12:21:06Z",edited_at:a,crossposted_at:a,published_at:d,last_comment_at:"2021-10-05T09:25:51Z",reading_time_minutes:12,tag_list:"microservices, architecture",tags:["microservices","architecture"],body_html:"\u003Cp\u003EUnless you’ve been developing software in a cave, you’ve probably heard people sing the praises of microservices. They’re agile, simple, and an overall improvement on the \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmonolith-vs-microservices\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003Emonolith\u003C\u002Fa\u003E and service-oriented architecture days. But of course, with all the benefits of microservices comes a new set of challenges.\u003C\u002Fp\u003E\n\n\u003Cp\u003EIn this article, we’ll look at some microservices best practices. Plus, we’ll suggest a few proven ways to help you design, orchestrate, and secure your microservices architecture. By understanding these practices, you’ll have a head start on a successful project.  \u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"benefits-and-challenges-of-microservices\" href=\"#benefits-and-challenges-of-microservices\"\u003E\n  \u003C\u002Fa\u003E\n  Benefits and Challenges of Microservices\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EBefore we dive into microservices best practices, however, we should first talk about some of the \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003Ebenefits\u003C\u002Fa\u003E and challenges of microservices and why you would want to use them in the first place. \u003C\u002Fp\u003E\n\n\u003Cp\u003EBriefly, microservices are an improved software architecture that allow you to:\u003C\u002Fp\u003E\n\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EDeploy and scale faster\u003C\u002Fstrong\u003E. Smaller application domain responsibility allows for automation, leading to faster deployments and faster scaling.\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EReduce downtime\u003C\u002Fstrong\u003E. Limit the impact that one unavailable service has on your primary business function, improving your overall business uptime.\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EEnsure availability\u003C\u002Fstrong\u003E. Keep functionality between microservices discrete, limiting the impact when an instance goes down.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Cp\u003EOf course, with these benefits, we have a new set of challenges, including inter-service communication, \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fapi-gateway\u002Fbuilding-a-secure-api-gateway\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003Esecurity\u003C\u002Fa\u003E, and scalability.\u003C\u002Fp\u003E\n\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EInter-service communication\u003C\u002Fstrong\u003E. With a monolithic application, all of the modules can inherently talk to one another. You have one certificate to manage, and once a request is authenticated and authorized, it can traverse the code paths without issue. When you extract a function out of the monolith architecture to a microservices application, what was once an internal function call becomes an external API call requiring \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmicroservices-security-and-session-management\u002F\"\u003Eauthentication\u003C\u002Fa\u003E and authorization for that outside microservice.\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ESecurity layer\u003C\u002Fstrong\u003E. Authentication and authorization, in the monolith application, can be handled one time at the point of entry. With the transition to microservices, every microservice needs to perform some authentication and authorization to enforce access controls. It's not realistic to ask users to log in every time they use a different microservice, so a comprehensive auth strategy needs to be established.\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EScalability\u003C\u002Fstrong\u003E. Although microservices allow you to scale independent functionality quickly, doing so effectively requires good app management and even better tooling. The effectiveness of your scalability hinges on your microservice \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmicroservices-orchestration\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003Eorchestration platform\u003C\u002Fa\u003E, which we’ll talk about in more detail below.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"microservices-best-practices\" href=\"#microservices-best-practices\"\u003E\n  \u003C\u002Fa\u003E\n  Microservices Best Practices\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EWith that quick overview of the benefits and challenges of microservices, let’s now dive into some best practices. These best practices will help you create a robust, easy-to-manage, scalable, and secure system of intercommunicating microservices.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"1-small-application-domain\" href=\"#1-small-application-domain\"\u003E\n  \u003C\u002Fa\u003E\n  1. Small Application Domain\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EAdopting a microservices strategy requires embracing the \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSingle-responsibility_principle\"\u003Esingle responsibility principle\u003C\u002Fa\u003E. By limiting the scope of responsibility for any single service, we limit the negative impact of that service failing. If a single microservice is responsible for too much, its failure or unavailability will have a domino effect on the rest of the system. \u003C\u002Fp\u003E\n\n\u003Cp\u003EA _micro_service should be just that: micro. Keep the app domain of your microservices small, dedicated to one logical functionality. This will reduce the impact that a given microservice has if any issues arise. In addition, smaller services are simpler to maintain. The result is easier updating and faster development.\u003C\u002Fp\u003E\n\n\u003Cp\u003EWhat does this look like in practice? For example, let's assume that our microservice is an API server that accepts requests to fetch data, and an authorization token must accompany those requests. When you're just starting, this is the only microservice that requires an authorization token. Why not just make authentication and token generation part of the microservice? At first glance, the advantage is fewer moving parts, less to manage.\u003C\u002Fp\u003E\n\n\u003Cp\u003EOf course, there will come a day when you'll have other services that require an authorization token. You'll soon find your original microservice functioning as an API server \u003Cem\u003Eand\u003C\u002Fem\u003E an authentication server. If your API server goes down, then your authentication server goes down with it. With that, so does every other service that requires an authorization token.\u003C\u002Fp\u003E\n\n\u003Cp\u003EBe considerate of your future self: keep your microservices small.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"2-separation-of-data-storage\" href=\"#2-separation-of-data-storage\"\u003E\n  \u003C\u002Fa\u003E\n  2. Separation of Data Storage\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EMultiple microservices connecting to the same database are still, in essence, a monolithic architecture. The monolith is just at the database layer instead of the application layer, making it just as fragile. Each microservice should have, as much as possible, its own data persistence layer. This not only ensures isolation from other microservices but also minimizes the blast radius if that particular data set were to become unavailable.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAt times, it might seem to make sense for different microservices to access data in the same database. However, a deeper examination might reveal that one microservice only works with a subset of database tables, while the other microservice only works with a completely different subset of tables. If the two subsets of data are completely orthogonal, this would be a good case for separating the database into separate services. This way, a single service depends on its dedicated data store, and that data store's failure will not impact any service besides that one.\u003C\u002Fp\u003E\n\n\u003Cp\u003EWe could make an analogous case for file stores. When adopting a microservices \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmicroservices-architectures\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003Earchitecture\u003C\u002Fa\u003E, there's no requirement for separate microservices to use the same file storage service. Unless there's an actual overlap of files, separate microservices ought to have separate file stores.\u003C\u002Fp\u003E\n\n\u003Cp\u003EWith this separation of data comes an increase in flexibility. For example, let's assume we had two microservices, both sharing the same file storage service with a cloud provider. One microservice regularly touches numerous assets but is small in file size. The other microservice has only a few files that it touches periodically, but those files are hundreds of gigabytes in size.\u003C\u002Fp\u003E\n\n\u003Cp\u003EUsing a common file store service for both microservices makes you less flexible to optimize costs since you have a mix of large and small files and a mix of regular and periodic access. If each microservice had its own data persistence layer—and that could be a separate microservice, of course—then you’d have more flexibility to find the provider or service that best fits the needs of that individual microservice.\u003C\u002Fp\u003E\n\n\u003Cp\u003ECost optimization, the flexibility of options, and less dependence on a single solution that could fail—these are all reasons to separate the data of different microservices.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"3-communication-channels\" href=\"#3-communication-channels\"\u003E\n  \u003C\u002Fa\u003E\n  3. Communication Channels\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EHow microservices communicate with one another—in particular, regarding events of interest—requires thoughtful consideration. Otherwise, a single unavailable service can lead to a communication breakdown that collapses an entire application.\u003C\u002Fp\u003E\n\n\u003Cp\u003EImagine a system of microservices for an online store. One microservice takes orders placed by a website. Another microservice sends a text notification to the customer that it received their order. Another microservice notifies the warehouse to send out the product. Finally, another microservice updates inventory counts.\u003C\u002Fp\u003E\n\n\u003Cp\u003EThere are two types of communication between microservices: synchronous and asynchronous. If we approach the above example using synchronous communication, a web server might process a new order by first sending a request to the customer notification service. After the customer notification service responds, the web server sends a request to the warehouse notification service, and again it waits for a response. Last, the web server sends a request to the inventory updater. Our synchronous approach would look like this:\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--4wu6jteg--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002Feeji10e3o0i1ty4jurav.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--4wu6jteg--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002Feeji10e3o0i1ty4jurav.png\" alt=\"Picture1\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003ESynchronous communication between microservices\u003C\u002Fp\u003E\n\n\u003Cp\u003EOf course, suppose the customer notification service happened to be down. In that case, the request to notify the customer might timeout or return an error or perhaps leave the web server waiting for a response indefinitely. The warehouse notification service might never get the request to fulfill the shipment. Synchronous communication between microservices can create a dependency chain that breaks if any link in the chain breaks.\u003C\u002Fp\u003E\n\n\u003Cp\u003EIn asynchronous communication, a service sends a request and continues its life without waiting for a response. In one possible asynchronous approach, the web server might send the \"notify customer\" request and then complete its task. The customer notification service is responsible for notifying the customer and sending an asynchronous request to the warehouse notification service, which is responsible for sending a request to the inventory updater service. It might look like this:\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--i8FIegZL--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002Ftm65gxrzmebhiqtuc84l.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--i8FIegZL--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002Ftm65gxrzmebhiqtuc84l.png\" alt=\"Picture2\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003EChained asynchronous communication between microservices\u003C\u002Fp\u003E\n\n\u003Cp\u003EIn this model, of course, we see that asynchronous communication can still result in a chain dependency, and the failure of a single service would still disrupt the application.\u003C\u002Fp\u003E\n\n\u003Cp\u003EA simple but effective approach to asynchronous communication is to adopt the publish\u002Fsubscribe pattern. When an event of interest occurs, the producer—in this case, the microservice—publishes a record of that event to a message queue service. Any other microservices interested in that type of event subscribe to the message queue service as consumers of that event. Microservices only talk to and listen to the message queue service, not each other.\u003C\u002Fp\u003E\n\n\u003Cp\u003EFor our example, it might look like this:\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--VPS2ZxnY--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002F1d87zza7w9a986l6zeff.png\" class=\"article-body-image-wrapper\"\u003E\u003Cimg src=\"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--VPS2ZxnY--\u002Fc_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002F1d87zza7w9a986l6zeff.png\" alt=\"Picture3\" loading=\"lazy\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\n\u003Cp\u003EAsynchronous communication facilitated by a message queue service\u003C\u002Fp\u003E\n\n\u003Cp\u003EThe message queue is a separate service of its own, decoupled from all of the microservices. It is in charge of receiving published events and notifying subscribers of those events. This ensures that the failure of one microservice, which might mean delayed delivery of a message, has minimal impact on other related but unconcerned services.\u003C\u002Fp\u003E\n\n\u003Cp\u003EThere are many tools to accomplish this kind of asynchronous communication (for example, Kafka or RabbitMQ). Look for ways to integrate tools like these as asynchronous communication backbones for your microservices.\u003C\u002Fp\u003E\n\n\u003Cp\u003EThere are cases when synchronous communication between microservices is necessary. Most request-response interactions are, out of necessity, synchronous. For example, an API server querying a database must wait for the query response; a web server fetching cached data must wait for the key-value store to respond. \u003C\u002Fp\u003E\n\n\u003Cp\u003EWhen synchronous communication is needed, you’ll want to use the open source \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Fkong\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003EKong Gateway\u003C\u002Fa\u003E to ensure that your communication is routed quickly and reliably to the right microservices.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"4-compatibility\" href=\"#4-compatibility\"\u003E\n  \u003C\u002Fa\u003E\n  4. Compatibility\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EAs much as possible, maintain backward compatibility, so your consumers don’t encounter broken APIs. The popular way to do this is by following path level compatibility guarantees like \u003Ccode\u003E\u002Fapi\u002Fv1\u003C\u002Fcode\u003E or \u003Ccode\u003E\u002Fapi\u002Fv2\u003C\u002Fcode\u003E. Any backward-incompatible changes go to a new path like \u003Ccode\u003E\u002Fapi\u002Fv3\u003C\u002Fcode\u003E. \u003C\u002Fp\u003E\n\n\u003Cp\u003EHowever, despite our best efforts as software engineers, sometimes we need to deprecate APIs, so we’re not stuck running them forever. With the \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Fblog\u002Fapi-gateway-request-transformation\u002F\"\u003EAPI gateway request transformation\u003C\u002Fa\u003E plugin, your microservices can alert your API consumers by easily injecting deprecation notices alongside the original API response or attaching a “deprecation header” similar to \u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Freference\u002Fusing-api\u002Fdeprecation-policy\u002F#rest-resources-aka-api-objects\"\u003EKubernetes\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"5-orchestrating-microservices\" href=\"#5-orchestrating-microservices\"\u003E\n  \u003C\u002Fa\u003E\n  5. Orchestrating Microservices\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EOrchestration of your microservices is a key factor of success in both process and tooling. Technically, you could use something like \u003Ccode\u003Esystemd\u003C\u002Fcode\u003E and Docker or \u003Ccode\u003Epodman\u003C\u002Fcode\u003E to run containers on a virtual machine, but that doesn’t provide the same level of resiliency as a container orchestration platform. This negatively affects the uptime and availability benefits that come with adopting a microservices architecture. For effective microservice orchestration, you’ll want to rely on a battle-tested container orchestration platform; and the clear leader in that field is \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fkubernetes\u002Fwhat-is-kubernetes\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003EKubernetes\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Cp\u003EKubernetes manages all of your containers’ provisioning and deployment while handling load balancing, scaling, replica sets for high availability, and network communication concerns.\u003C\u002Fp\u003E\n\n\u003Cp\u003EYou might deploy bare Kubernetes on-premises, or you might go with a cloud distribution like Azure Kubernetes Service, Red Hat OpenShift, or Amazon Elastic Kubernetes Service. The built-in scheduling, replication, and networking capabilities of Kubernetes make microservice orchestration much easier than on a traditional operating system. \u003C\u002Fp\u003E\n\n\u003Cp\u003ECouple Kubernetes with \u003Ca href=\"https:\u002F\u002Fkuma.io\u002F\"\u003EKuma\u003C\u002Fa\u003E service mesh and \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Fsolutions\u002Fkubernetes-ingress\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003EKong Ingress Controller\u003C\u002Fa\u003E, and you have microservices that are discoverable, monitored, and resilient—like magic.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"6-microservices-security\" href=\"#6-microservices-security\"\u003E\n  \u003C\u002Fa\u003E\n  6. Microservices Security\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EAs your application comprises more and more microservices, ensuring proper security can become a complicated beast. A centralized system for enforcing security policies is vital to protecting your overall application from malicious users, invasive bots, and faulty code. \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002F?utm_source=guest&amp;utm_medium=devspotlight&amp;utm_campaign=community\"\u003EKong\u003C\u002Fa\u003E ought to be the start of your security story with microservices, whether you’re running on VMs or in Kubernetes. The abundance of Kong-maintained \u003Ca href=\"https:\u002F\u002Fdocs.konghq.com\u002Fhub\u002F#security\"\u003Esecurity plugins\u003C\u002Fa\u003E makes it easy to address some of the most common needs for microservices, including \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Fblog\u002Fkong-gateway-key-authentication\u002F\"\u003Eauthentication\u003C\u002Fa\u003E, authorization, traffic control, and \u003Ca href=\"https:\u002F\u002Fkonghq.com\u002Fblog\u002Fkong-gateway-rate-limiting\u002F\"\u003Erate limiting\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Ch3\u003E\n  \u003Ca name=\"example-rate-limiting-with-kong-ingress-controller\" href=\"#example-rate-limiting-with-kong-ingress-controller\"\u003E\n  \u003C\u002Fa\u003E\n  Example: Rate Limiting with Kong Ingress Controller\n\u003C\u002Fh3\u003E\n\n\u003Cp\u003ETo demonstrate an example of a security plugin at work, we'll deploy Kong's \u003Ca href=\"https:\u002F\u002Fdocs.konghq.com\u002Fhub\u002Fkong-inc\u002Frate-limiting\u002F\"\u003ERate Limiting plugin\u003C\u002Fa\u003E to show how Kong can prevent excessive inbound requests to your applications. We'll create a local Kubernetes cluster with \u003Ccode\u003E\u003Ca href=\"https:\u002F\u002Fkind.sigs.k8s.io\u002F\"\u003Ekind\u003C\u002Fa\u003E\u003C\u002Fcode\u003E and then deploy the Kong Ingress Controller by following these \u003Ca href=\"https:\u002F\u002Fdocs.konghq.com\u002Fkubernetes-ingress-controller\u002F1.3.x\u002Fdeployment\u002Fk4k8s\u002F\"\u003Einstructions\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAfter creating a cluster and deploying the Kong Ingress Controller, our first step is to set up the Rate Limiting plugin. There are different scopes for which you can set up the plugin. We’ll use the default project on our Kubernetes cluster for our use case and scope the plugin to that default namespace.\u003Cbr\u003E\n\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003E$ echo 'apiVersion: configuration.konghq.com\u002Fv1\nkind: KongPlugin\nmetadata:\n  name: rate-limiting-example\n  namespace: default\nconfig:\n  second: 5\n  hour: 10000\n  policy: local\nplugin: rate-limiting' | kubectl apply -f -\nkongplugin.configuration.konghq.com\u002Frate-limiting-example created\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\n\n\u003Cp\u003ENow, we’ll create an “echo service” and an ingress for the service. In this case, we’re borrowing the example from Kong’s \u003Ca href=\"https:\u002F\u002Fdocs.konghq.com\u002Fkubernetes-ingress-controller\u002F1.3.x\u002Fguides\u002Fgetting-started\u002F#set-up-an-echo-server\"\u003EGetting Started with Kubernetes Ingress Controller\u003C\u002Fa\u003E documentation:\u003Cbr\u003E\n\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003E$ kubectl apply -f https:\u002F\u002Fbit.ly\u002Fecho-service\nservice\u002Fecho created\ndeployment.apps\u002Fecho created\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\n\n\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003E$ echo \"\napiVersion: extensions\u002Fv1beta1\nkind: Ingress\nmetadata:\n  name: demo\n  annotations:\n    kubernetes.io\u002Fingress.class: kong\n    konghq.com\u002Fplugins: rate-limiting-example\nspec:\n  rules:\n  - http:\n      paths:\n      - path: \u002Ffoo\n        backend:\n          serviceName: echo\n          servicePort: 80\n\" | kubectl apply -f -\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\n\n\u003Cp\u003EThe last thing we need to do is test! We’ll borrow the \u003Ccode\u003Eshell-demo\u003C\u002Fcode\u003E from the Kubernetes\u003Ca href=\"https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Ftasks\u002Fdebug-application-cluster\u002Fget-shell-running-container\u002F\"\u003E documentation for in-cluster testing\u003C\u002Fa\u003E:\u003Cbr\u003E\n\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003E$ kubectl apply -f https:\u002F\u002Fk8s.io\u002Fexamples\u002Fapplication\u002Fshell-demo.yaml -n default\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\n\n\u003Cp\u003EBefore getting into our shell pod, we’ll need the cluster IP of \u003Ccode\u003Ekong-proxy\u003C\u002Fcode\u003E:\u003Cbr\u003E\n\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003E$ kubectl get svc\u002Fkong-proxy -n kong -o jsonpath='{.spec.clusterIP}'\n10.96.74.69\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\n\n\u003Cp\u003ENow, we can get shell access to our pod and test the rate limiting:\u003Cbr\u003E\n\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"highlight js-code-highlight\"\u003E\n\u003Cpre class=\"highlight plaintext\"\u003E\u003Ccode\u003E$ kubectl exec --stdin --tty shell-demo -- \u002Fbin\u002Fbash\n# curl -I 10.96.74.69\u002Ffoo\nHTTP\u002F1.1 200 OK\nContent-Type: text\u002Fplain; charset=UTF-8\nConnection: keep-alive\nX-RateLimit-Limit-Second: 5\nX-RateLimit-Remaining-Hour: 9998\nX-RateLimit-Limit-Hour: 10000\nRateLimit-Reset: 1\nRateLimit-Remaining: 4\nRateLimit-Limit: 5\nX-RateLimit-Remaining-Second: 4\nDate: Sat, 24 Jul 2021 20:01:35 GMT\nServer: echoserver\nX-Kong-Upstream-Latency: 0\nX-Kong-Proxy-Latency: 0\nVia: kong\u002F2.4.1\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"highlight__panel js-actions-panel\"\u003E\n\u003Cdiv class=\"highlight__panel-action js-fullscreen-code-action\"\u003E\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"\u003E\u003Ctitle\u003EEnter fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n    \u003Csvg xmlns=\"http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"\u003E\u003Ctitle\u003EExit fullscreen mode\u003C\u002Ftitle\u003E\n    \u003Cpath d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"\u003E\u003C\u002Fpath\u003E\n\u003C\u002Fsvg\u003E\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\n\n\u003Cp\u003EThe additional step of using an intermediary pod to test the rate limiting won’t be necessary for most cloud providers, which gives you a load balancer out of the box. In this case, since we’re using \u003Ccode\u003Ekind\u003C\u002Fcode\u003E, there’s no load balancer provisioned, so our test comes from within the cluster. This same test would work externally if a load balancer were available.\u003C\u002Fp\u003E\n\n\u003Cp\u003ERate limiting is just one example of where Kong fits into the security concerns of your overall microservices strategy and best practices but can easily provide a fully comprehensive solution. Kong maintains several plugins and products to keep your communication channels bulletproof, API change impact minimal, and your application domains manageable. Plus, it's compatible with most programming languages and vendor options.\u003C\u002Fp\u003E\n\n\u003Ch2\u003E\n  \u003Ca name=\"7-metrics-and-monitoring\" href=\"#7-metrics-and-monitoring\"\u003E\n  \u003C\u002Fa\u003E\n  7. Metrics and Monitoring\n\u003C\u002Fh2\u003E\n\n\u003Cp\u003EAn architecture built on microservices can lead to massive scaling of hundreds or thousands of small, modular services. While that yields huge potential for increased speed, availability, and reach, a sprawling system of microservices requires a strategic and systematic approach to monitoring. By keeping an eye on all of your microservices, you'll ensure that they are functioning as they ought to, are available to your users, and are using resources appropriately. When any of these expectations are not met, you can respond by taking proper action.\u003C\u002Fp\u003E\n\n\u003Cp\u003EFortunately, you don't need to reinvent the wheel when it comes to monitoring. There are several widely adopted monitoring solutions that can integrate seamlessly within your infrastructure. Some solutions use metrics exporter SDKs which can be integrated by adding one or two lines of code in your microservice. Others can be integrated with your API gateway or service mesh as a plugin, for monitoring networking concerns and resource usage.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAs your monitoring tools gather metrics, those metrics can be consumed by visualization tools—beautiful dashboards that help you see the numbers behind your microservices. How many users were online last Thursday at 8:00 PM? How much has CPU load increased since we released that new feature? What's the latency between our product shipping API and the invoicing API?\u003C\u002Fp\u003E\n\n\u003Cp\u003EBy monitoring your microservices and having your hard numbers presented clearly, you're equipped to make informed decisions about how to keep your microservices healthy and available. As you do that, you'll keep your users happy.\u003C\u002Fp\u003E\n\n\u003Ch1\u003E\n  \u003Ca name=\"dont-forget-your-lap-bar\" href=\"#dont-forget-your-lap-bar\"\u003E\n  \u003C\u002Fa\u003E\n  Don’t Forget Your Lap Bar...\n\u003C\u002Fh1\u003E\n\n\u003Cp\u003EMicroservices are a wild ride! You start with the incredible benefits of speedier deployment and scalability, reduced downtime, and overall improvement of your business availability. Then, you throw in your orchestration platform, along with some best practices powered by Kong and its plugins, and boom! You have a symphony of packets flowing to and fro between your microservices that are secure, reliable, and bulletproof. We’ve only covered a small subset of what Kong can do, so I’d highly recommend checking out \u003Ca href=\"https:\u002F\u002Fdocs.konghq.com\u002Fhub\u002F\"\u003EKong Hub\u003C\u002Fa\u003E to see all the functionality available to ease your journey to microservice nirvana!\u003C\u002Fp\u003E\n\n",body_markdown:"Unless you’ve been developing software in a cave, you’ve probably heard people sing the praises of microservices. They’re agile, simple, and an overall improvement on the [monolith](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmonolith-vs-microservices\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community) and service-oriented architecture days. But of course, with all the benefits of microservices comes a new set of challenges.\n\nIn this article, we’ll look at some microservices best practices. Plus, we’ll suggest a few proven ways to help you design, orchestrate, and secure your microservices architecture. By understanding these practices, you’ll have a head start on a successful project.  \n\n\n## Benefits and Challenges of Microservices\n\nBefore we dive into microservices best practices, however, we should first talk about some of the [benefits](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community) and challenges of microservices and why you would want to use them in the first place. \n\nBriefly, microservices are an improved software architecture that allow you to:\n\n\n\n* **Deploy and scale faster**. Smaller application domain responsibility allows for automation, leading to faster deployments and faster scaling.\n* **Reduce downtime**. Limit the impact that one unavailable service has on your primary business function, improving your overall business uptime.\n* **Ensure availability**. Keep functionality between microservices discrete, limiting the impact when an instance goes down.\n\nOf course, with these benefits, we have a new set of challenges, including inter-service communication, [security](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fapi-gateway\u002Fbuilding-a-secure-api-gateway\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community), and scalability.\n\n\n\n* **Inter-service communication**. With a monolithic application, all of the modules can inherently talk to one another. You have one certificate to manage, and once a request is authenticated and authorized, it can traverse the code paths without issue. When you extract a function out of the monolith architecture to a microservices application, what was once an internal function call becomes an external API call requiring [authentication](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmicroservices-security-and-session-management\u002F) and authorization for that outside microservice.\n* **Security layer**. Authentication and authorization, in the monolith application, can be handled one time at the point of entry. With the transition to microservices, every microservice needs to perform some authentication and authorization to enforce access controls. It's not realistic to ask users to log in every time they use a different microservice, so a comprehensive auth strategy needs to be established.\n* **Scalability**. Although microservices allow you to scale independent functionality quickly, doing so effectively requires good app management and even better tooling. The effectiveness of your scalability hinges on your microservice [orchestration platform](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmicroservices-orchestration\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community), which we’ll talk about in more detail below.\n\n\n# Microservices Best Practices\n\nWith that quick overview of the benefits and challenges of microservices, let’s now dive into some best practices. These best practices will help you create a robust, easy-to-manage, scalable, and secure system of intercommunicating microservices.\n\n\n## 1. Small Application Domain\n\nAdopting a microservices strategy requires embracing the [single responsibility principle](https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSingle-responsibility_principle). By limiting the scope of responsibility for any single service, we limit the negative impact of that service failing. If a single microservice is responsible for too much, its failure or unavailability will have a domino effect on the rest of the system. \n\nA _micro_service should be just that: micro. Keep the app domain of your microservices small, dedicated to one logical functionality. This will reduce the impact that a given microservice has if any issues arise. In addition, smaller services are simpler to maintain. The result is easier updating and faster development.\n\nWhat does this look like in practice? For example, let's assume that our microservice is an API server that accepts requests to fetch data, and an authorization token must accompany those requests. When you're just starting, this is the only microservice that requires an authorization token. Why not just make authentication and token generation part of the microservice? At first glance, the advantage is fewer moving parts, less to manage.\n\nOf course, there will come a day when you'll have other services that require an authorization token. You'll soon find your original microservice functioning as an API server _and_ an authentication server. If your API server goes down, then your authentication server goes down with it. With that, so does every other service that requires an authorization token.\n\nBe considerate of your future self: keep your microservices small.\n\n\n## 2. Separation of Data Storage\n\nMultiple microservices connecting to the same database are still, in essence, a monolithic architecture. The monolith is just at the database layer instead of the application layer, making it just as fragile. Each microservice should have, as much as possible, its own data persistence layer. This not only ensures isolation from other microservices but also minimizes the blast radius if that particular data set were to become unavailable.\n\nAt times, it might seem to make sense for different microservices to access data in the same database. However, a deeper examination might reveal that one microservice only works with a subset of database tables, while the other microservice only works with a completely different subset of tables. If the two subsets of data are completely orthogonal, this would be a good case for separating the database into separate services. This way, a single service depends on its dedicated data store, and that data store's failure will not impact any service besides that one.\n\nWe could make an analogous case for file stores. When adopting a microservices [architecture](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fmicroservices\u002Fmicroservices-architectures\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community), there's no requirement for separate microservices to use the same file storage service. Unless there's an actual overlap of files, separate microservices ought to have separate file stores.\n\nWith this separation of data comes an increase in flexibility. For example, let's assume we had two microservices, both sharing the same file storage service with a cloud provider. One microservice regularly touches numerous assets but is small in file size. The other microservice has only a few files that it touches periodically, but those files are hundreds of gigabytes in size.\n\nUsing a common file store service for both microservices makes you less flexible to optimize costs since you have a mix of large and small files and a mix of regular and periodic access. If each microservice had its own data persistence layer—and that could be a separate microservice, of course—then you’d have more flexibility to find the provider or service that best fits the needs of that individual microservice.\n\nCost optimization, the flexibility of options, and less dependence on a single solution that could fail—these are all reasons to separate the data of different microservices.\n\n\n## 3. Communication Channels\n\nHow microservices communicate with one another—in particular, regarding events of interest—requires thoughtful consideration. Otherwise, a single unavailable service can lead to a communication breakdown that collapses an entire application.\n\nImagine a system of microservices for an online store. One microservice takes orders placed by a website. Another microservice sends a text notification to the customer that it received their order. Another microservice notifies the warehouse to send out the product. Finally, another microservice updates inventory counts.\n\nThere are two types of communication between microservices: synchronous and asynchronous. If we approach the above example using synchronous communication, a web server might process a new order by first sending a request to the customer notification service. After the customer notification service responds, the web server sends a request to the warehouse notification service, and again it waits for a response. Last, the web server sends a request to the inventory updater. Our synchronous approach would look like this:\n\n\n![Picture1](https:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002Feeji10e3o0i1ty4jurav.png)\n \n\n\nSynchronous communication between microservices\n\nOf course, suppose the customer notification service happened to be down. In that case, the request to notify the customer might timeout or return an error or perhaps leave the web server waiting for a response indefinitely. The warehouse notification service might never get the request to fulfill the shipment. Synchronous communication between microservices can create a dependency chain that breaks if any link in the chain breaks.\n\nIn asynchronous communication, a service sends a request and continues its life without waiting for a response. In one possible asynchronous approach, the web server might send the \"notify customer\" request and then complete its task. The customer notification service is responsible for notifying the customer and sending an asynchronous request to the warehouse notification service, which is responsible for sending a request to the inventory updater service. It might look like this:\n\n\n![Picture2](https:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002Ftm65gxrzmebhiqtuc84l.png)\n \n\nChained asynchronous communication between microservices\n\nIn this model, of course, we see that asynchronous communication can still result in a chain dependency, and the failure of a single service would still disrupt the application.\n\nA simple but effective approach to asynchronous communication is to adopt the publish\u002Fsubscribe pattern. When an event of interest occurs, the producer—in this case, the microservice—publishes a record of that event to a message queue service. Any other microservices interested in that type of event subscribe to the message queue service as consumers of that event. Microservices only talk to and listen to the message queue service, not each other.\n\nFor our example, it might look like this:\n\n\n![Picture3](https:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Farticles\u002F1d87zza7w9a986l6zeff.png)\n \n\nAsynchronous communication facilitated by a message queue service\n\nThe message queue is a separate service of its own, decoupled from all of the microservices. It is in charge of receiving published events and notifying subscribers of those events. This ensures that the failure of one microservice, which might mean delayed delivery of a message, has minimal impact on other related but unconcerned services.\n\nThere are many tools to accomplish this kind of asynchronous communication (for example, Kafka or RabbitMQ). Look for ways to integrate tools like these as asynchronous communication backbones for your microservices.\n\nThere are cases when synchronous communication between microservices is necessary. Most request-response interactions are, out of necessity, synchronous. For example, an API server querying a database must wait for the query response; a web server fetching cached data must wait for the key-value store to respond. \n\nWhen synchronous communication is needed, you’ll want to use the open source [Kong Gateway](https:\u002F\u002Fkonghq.com\u002Fkong\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community) to ensure that your communication is routed quickly and reliably to the right microservices.\n\n\n## 4. Compatibility\n\nAs much as possible, maintain backward compatibility, so your consumers don’t encounter broken APIs. The popular way to do this is by following path level compatibility guarantees like `\u002Fapi\u002Fv1` or `\u002Fapi\u002Fv2`. Any backward-incompatible changes go to a new path like `\u002Fapi\u002Fv3`. \n\nHowever, despite our best efforts as software engineers, sometimes we need to deprecate APIs, so we’re not stuck running them forever. With the [API gateway request transformation](https:\u002F\u002Fkonghq.com\u002Fblog\u002Fapi-gateway-request-transformation\u002F) plugin, your microservices can alert your API consumers by easily injecting deprecation notices alongside the original API response or attaching a “deprecation header” similar to [Kubernetes](https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Freference\u002Fusing-api\u002Fdeprecation-policy\u002F#rest-resources-aka-api-objects).\n\n\n## 5. Orchestrating Microservices\n\nOrchestration of your microservices is a key factor of success in both process and tooling. Technically, you could use something like `systemd` and Docker or `podman` to run containers on a virtual machine, but that doesn’t provide the same level of resiliency as a container orchestration platform. This negatively affects the uptime and availability benefits that come with adopting a microservices architecture. For effective microservice orchestration, you’ll want to rely on a battle-tested container orchestration platform; and the clear leader in that field is [Kubernetes](https:\u002F\u002Fkonghq.com\u002Flearning-center\u002Fkubernetes\u002Fwhat-is-kubernetes\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community).\n\nKubernetes manages all of your containers’ provisioning and deployment while handling load balancing, scaling, replica sets for high availability, and network communication concerns.\n\nYou might deploy bare Kubernetes on-premises, or you might go with a cloud distribution like Azure Kubernetes Service, Red Hat OpenShift, or Amazon Elastic Kubernetes Service. The built-in scheduling, replication, and networking capabilities of Kubernetes make microservice orchestration much easier than on a traditional operating system. \n\nCouple Kubernetes with [Kuma](https:\u002F\u002Fkuma.io\u002F) service mesh and [Kong Ingress Controller](https:\u002F\u002Fkonghq.com\u002Fsolutions\u002Fkubernetes-ingress\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community), and you have microservices that are discoverable, monitored, and resilient—like magic.\n\n\n## 6. Microservices Security\n\nAs your application comprises more and more microservices, ensuring proper security can become a complicated beast. A centralized system for enforcing security policies is vital to protecting your overall application from malicious users, invasive bots, and faulty code. [Kong](https:\u002F\u002Fkonghq.com\u002F?utm_source=guest&utm_medium=devspotlight&utm_campaign=community) ought to be the start of your security story with microservices, whether you’re running on VMs or in Kubernetes. The abundance of Kong-maintained [security plugins](https:\u002F\u002Fdocs.konghq.com\u002Fhub\u002F#security) makes it easy to address some of the most common needs for microservices, including [authentication](https:\u002F\u002Fkonghq.com\u002Fblog\u002Fkong-gateway-key-authentication\u002F), authorization, traffic control, and [rate limiting](https:\u002F\u002Fkonghq.com\u002Fblog\u002Fkong-gateway-rate-limiting\u002F).\n\n\n### Example: Rate Limiting with Kong Ingress Controller\n\nTo demonstrate an example of a security plugin at work, we'll deploy Kong's [Rate Limiting plugin](https:\u002F\u002Fdocs.konghq.com\u002Fhub\u002Fkong-inc\u002Frate-limiting\u002F) to show how Kong can prevent excessive inbound requests to your applications. We'll create a local Kubernetes cluster with \u003Ccode\u003E[kind](https:\u002F\u002Fkind.sigs.k8s.io\u002F)\u003C\u002Fcode\u003E and then deploy the Kong Ingress Controller by following these [instructions](https:\u002F\u002Fdocs.konghq.com\u002Fkubernetes-ingress-controller\u002F1.3.x\u002Fdeployment\u002Fk4k8s\u002F).\n\nAfter creating a cluster and deploying the Kong Ingress Controller, our first step is to set up the Rate Limiting plugin. There are different scopes for which you can set up the plugin. We’ll use the default project on our Kubernetes cluster for our use case and scope the plugin to that default namespace.\n\n\n```\n$ echo 'apiVersion: configuration.konghq.com\u002Fv1\nkind: KongPlugin\nmetadata:\n  name: rate-limiting-example\n  namespace: default\nconfig:\n  second: 5\n  hour: 10000\n  policy: local\nplugin: rate-limiting' | kubectl apply -f -\nkongplugin.configuration.konghq.com\u002Frate-limiting-example created\n```\n\n\nNow, we’ll create an “echo service” and an ingress for the service. In this case, we’re borrowing the example from Kong’s [Getting Started with Kubernetes Ingress Controller](https:\u002F\u002Fdocs.konghq.com\u002Fkubernetes-ingress-controller\u002F1.3.x\u002Fguides\u002Fgetting-started\u002F#set-up-an-echo-server) documentation:\n\n\n```\n$ kubectl apply -f https:\u002F\u002Fbit.ly\u002Fecho-service\nservice\u002Fecho created\ndeployment.apps\u002Fecho created\n```\n\n\n\n```\n$ echo \"\napiVersion: extensions\u002Fv1beta1\nkind: Ingress\nmetadata:\n  name: demo\n  annotations:\n    kubernetes.io\u002Fingress.class: kong\n    konghq.com\u002Fplugins: rate-limiting-example\nspec:\n  rules:\n  - http:\n      paths:\n      - path: \u002Ffoo\n        backend:\n          serviceName: echo\n          servicePort: 80\n\" | kubectl apply -f -\n```\n\n\nThe last thing we need to do is test! We’ll borrow the `shell-demo` from the Kubernetes[ documentation for in-cluster testing](https:\u002F\u002Fkubernetes.io\u002Fdocs\u002Ftasks\u002Fdebug-application-cluster\u002Fget-shell-running-container\u002F):\n\n\n```\n$ kubectl apply -f https:\u002F\u002Fk8s.io\u002Fexamples\u002Fapplication\u002Fshell-demo.yaml -n default\n```\n\n\nBefore getting into our shell pod, we’ll need the cluster IP of `kong-proxy`:\n\n\n```\n$ kubectl get svc\u002Fkong-proxy -n kong -o jsonpath='{.spec.clusterIP}'\n10.96.74.69\n```\n\n\nNow, we can get shell access to our pod and test the rate limiting:\n\n\n```\n$ kubectl exec --stdin --tty shell-demo -- \u002Fbin\u002Fbash\n# curl -I 10.96.74.69\u002Ffoo\nHTTP\u002F1.1 200 OK\nContent-Type: text\u002Fplain; charset=UTF-8\nConnection: keep-alive\nX-RateLimit-Limit-Second: 5\nX-RateLimit-Remaining-Hour: 9998\nX-RateLimit-Limit-Hour: 10000\nRateLimit-Reset: 1\nRateLimit-Remaining: 4\nRateLimit-Limit: 5\nX-RateLimit-Remaining-Second: 4\nDate: Sat, 24 Jul 2021 20:01:35 GMT\nServer: echoserver\nX-Kong-Upstream-Latency: 0\nX-Kong-Proxy-Latency: 0\nVia: kong\u002F2.4.1\n```\n\n\nThe additional step of using an intermediary pod to test the rate limiting won’t be necessary for most cloud providers, which gives you a load balancer out of the box. In this case, since we’re using `kind`, there’s no load balancer provisioned, so our test comes from within the cluster. This same test would work externally if a load balancer were available.\n\nRate limiting is just one example of where Kong fits into the security concerns of your overall microservices strategy and best practices but can easily provide a fully comprehensive solution. Kong maintains several plugins and products to keep your communication channels bulletproof, API change impact minimal, and your application domains manageable. Plus, it's compatible with most programming languages and vendor options.\n\n\n## 7. Metrics and Monitoring\n\nAn architecture built on microservices can lead to massive scaling of hundreds or thousands of small, modular services. While that yields huge potential for increased speed, availability, and reach, a sprawling system of microservices requires a strategic and systematic approach to monitoring. By keeping an eye on all of your microservices, you'll ensure that they are functioning as they ought to, are available to your users, and are using resources appropriately. When any of these expectations are not met, you can respond by taking proper action.\n\nFortunately, you don't need to reinvent the wheel when it comes to monitoring. There are several widely adopted monitoring solutions that can integrate seamlessly within your infrastructure. Some solutions use metrics exporter SDKs which can be integrated by adding one or two lines of code in your microservice. Others can be integrated with your API gateway or service mesh as a plugin, for monitoring networking concerns and resource usage.\n\nAs your monitoring tools gather metrics, those metrics can be consumed by visualization tools—beautiful dashboards that help you see the numbers behind your microservices. How many users were online last Thursday at 8:00 PM? How much has CPU load increased since we released that new feature? What's the latency between our product shipping API and the invoicing API?\n\nBy monitoring your microservices and having your hard numbers presented clearly, you're equipped to make informed decisions about how to keep your microservices healthy and available. As you do that, you'll keep your users happy.\n\n\n# Don’t Forget Your Lap Bar...\n\nMicroservices are a wild ride! You start with the incredible benefits of speedier deployment and scalability, reduced downtime, and overall improvement of your business availability. Then, you throw in your orchestration platform, along with some best practices powered by Kong and its plugins, and boom! You have a symphony of packets flowing to and fro between your microservices that are secure, reliable, and bulletproof. We’ve only covered a small subset of what Kong can do, so I’d highly recommend checking out [Kong Hub](https:\u002F\u002Fdocs.konghq.com\u002Fhub\u002F) to see all the functionality available to ease your journey to microservice nirvana!\n",user:{name:"Michael Bogan",username:"mbogan",twitter_username:"mmbogan",github_username:"CapnMB",website_url:a,profile_image:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--vOitxlJp--\u002Fc_fill,f_auto,fl_progressive,h_640,q_auto,w_640\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F179748\u002F3068c723-39a7-4439-96b5-9cc6a58ccd96.jpg",profile_image_90:"https:\u002F\u002Fres.cloudinary.com\u002Fpracticaldev\u002Fimage\u002Ffetch\u002Fs--IeuWQBnv--\u002Fc_fill,f_auto,fl_progressive,h_90,q_auto,w_90\u002Fhttps:\u002F\u002Fdev-to-uploads.s3.amazonaws.com\u002Fuploads\u002Fuser\u002Fprofile_image\u002F179748\u002F3068c723-39a7-4439-96b5-9cc6a58ccd96.jpg"}}},serverRendered:true,routePath:"\u002Farticles\u002Fmbogan\u002F822434",config:{_app:{basePath:"\u002F",assetsPath:"\u002F_nuxt\u002F",cdnURL:a}}}}(null,"https:\u002F\u002Fdev.to\u002Fmbogan\u002F7-microservices-best-practices-for-developers-ec5",231,"2021-09-13T12:21:05Z"));